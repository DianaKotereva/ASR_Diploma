{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cd6d4a2",
   "metadata": {
    "cellId": "hx4ilavsekb7p5j4apa0ad"
   },
   "source": [
    "# Импорты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4259695f",
   "metadata": {
    "cellId": "cnf87s891veud9gt2lxxl"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import scipy.io.wavfile as wav\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbdb32a2",
   "metadata": {
    "cellId": "oc77e6o7t1qsutuvez776"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import transformers\n",
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7a3fd91",
   "metadata": {
    "cellId": "75nk4sdqcfvl8a494j8ev"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from models import ConvFeatureEncoder, SegmentsRepr, SegmentsEncoder, NegativeSampler, SegmentPredictor, FinModel\n",
    "from utils import ConstrativeLoss, sample_negatives\n",
    "from collections import OrderedDict\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e827b8df",
   "metadata": {
    "cellId": "dlkze2hpusrdlri7x2hivr"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from model_transformers import SegmentTransformer\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4756c29",
   "metadata": {
    "cellId": "lkg6h7gwm2bgzpcn9nzv8s"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.14.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "677d7402",
   "metadata": {
    "cellId": "qmml82lztvsz43lm9lnsja"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "class Dataset:\n",
    "    \n",
    "    def __init__(self, path, segment_path, manifest_path, edges_path = None, chars = 'segments_chars/', frames = 'secs/'):\n",
    "        with open(manifest_path, 'r') as json_file:\n",
    "            manifest = json.load(json_file)\n",
    "        self.manifest = manifest\n",
    "#         self.manifest = manifest[:1000]\n",
    "        self.path = path\n",
    "        self.segment_path = segment_path\n",
    "        self.frames = os.path.join(chars, frames)\n",
    "        self.edges_path = edges_path\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.manifest)\n",
    "    \n",
    "    def __getitem__(self, ind):\n",
    "#         print(self.manifest[ind])\n",
    "        \n",
    "        # Загрузка аудио-сигнала\n",
    "        \n",
    "        audio_filepath = self.manifest[ind]['audio_filepath']\n",
    "#         print(audio_filepath)\n",
    "        audio_file = os.path.join(self.path, audio_filepath)\n",
    "#         print(audio_file)\n",
    "        sampling_rate, signal = wav.read(audio_file)\n",
    "#         signal, sampling_rate = torchaudio.load(path)\n",
    "        \n",
    "        # Обрезать тишину\n",
    "        filetext_id = self.manifest[ind]['id']+'.txt'\n",
    "        silero_filepath = audio_filepath.replace('.wav', '.txt')\n",
    "        silero_file = os.path.join(self.edges_path, silero_filepath)\n",
    "#         print(silero_file)\n",
    "        \n",
    "        with open(silero_file, 'r', encoding=\"cp1251\") as file:\n",
    "            tt = file.read()\n",
    "        \n",
    "        start = []\n",
    "        end = []\n",
    "        for line in tt.split('\\n'):\n",
    "            if len(line) > 0:\n",
    "                start.append(eval(line.split()[0]))\n",
    "                end.append(eval(line.split()[1]))\n",
    "        start = min(start)\n",
    "        end = max(end)\n",
    "#         print(start)\n",
    "#         print(end)\n",
    "        signal = signal[start:end]\n",
    "        \n",
    "        # Загрузка разметки\n",
    "        filetext_id = self.manifest[ind]['id']+'.txt'\n",
    "        segment_filepath = audio_filepath.replace('.wav', '.txt').replace(filetext_id, '')\n",
    "        \n",
    "        segment_file = os.path.join(os.path.join(os.path.join(self.segment_path, segment_filepath), self.frames), filetext_id)\n",
    "#         print(segment_file)\n",
    "        with open(segment_file, 'r', encoding=\"cp1251\") as file:\n",
    "            tt = file.read()\n",
    "        \n",
    "        boundaries = set()\n",
    "        mm = [i for i in tt.split('\\n') if len(i)>0]\n",
    "        for i in mm:\n",
    "            boundaries.add(eval(i.split()[0]))\n",
    "            boundaries.add(eval(i.split()[1]))\n",
    "        boundaries = sorted(list(boundaries))\n",
    "        \n",
    "        return {'audio_file':os.path.join(self.path, audio_filepath), \n",
    "                'segment_file':segment_file, \n",
    "                'id':filetext_id, 'sample': signal, \n",
    "                'length': len(signal), 'boundaries': boundaries}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9eba5762",
   "metadata": {
    "cellId": "gfwg7tmhh5bzsaq582fpm"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "class Dataset_test:\n",
    "    \n",
    "    def __init__(self, path, segment_path, manifest_path, edges_path = None, chars = 'segments_chars/', frames = 'secs/'):\n",
    "        with open(manifest_path, 'r') as json_file:\n",
    "            manifest = json.load(json_file)\n",
    "        self.manifest = manifest\n",
    "#         self.manifest = manifest[:1000]\n",
    "        self.path = path\n",
    "        self.segment_path = segment_path\n",
    "        self.frames = os.path.join(chars, frames)\n",
    "        self.edges_path = edges_path\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.manifest)\n",
    "    \n",
    "    def __getitem__(self, ind):\n",
    "#         print(self.manifest[ind])\n",
    "        \n",
    "        # Загрузка аудио-сигнала\n",
    "        \n",
    "        audio_filepath = self.manifest[ind]['audio_filepath']\n",
    "#         print(audio_filepath)\n",
    "        audio_file = os.path.join(self.path, audio_filepath)\n",
    "#         print(audio_file)\n",
    "        sampling_rate, signal = wav.read(audio_file)\n",
    "#         signal, sampling_rate = torchaudio.load(path)\n",
    "        \n",
    "        # Обрезать тишину\n",
    "        filetext_id = self.manifest[ind]['id']+'.txt'\n",
    "        silero_filepath = audio_filepath.replace('.wav', '.txt')\n",
    "        silero_file = os.path.join(self.edges_path, silero_filepath).replace('files/', '')\n",
    "#         print(silero_file)\n",
    "        \n",
    "        with open(silero_file, 'r', encoding=\"cp1251\") as file:\n",
    "            tt = file.read()\n",
    "        \n",
    "        start = []\n",
    "        end = []\n",
    "        for line in tt.split('\\n'):\n",
    "            if len(line) > 0:\n",
    "                start.append(eval(line.split()[0]))\n",
    "                end.append(eval(line.split()[1]))\n",
    "        start = min(start)\n",
    "        end = max(end)\n",
    "#         print(start)\n",
    "#         print(end)\n",
    "        signal = signal[start:end]\n",
    "        \n",
    "        # Загрузка разметки\n",
    "        filetext_id = self.manifest[ind]['id']+'.txt'\n",
    "        segment_filepath = audio_filepath.replace('.wav', '.txt').replace(filetext_id, '')\n",
    "        \n",
    "        segment_file = os.path.join(os.path.join(os.path.join(self.segment_path, segment_filepath), self.frames), filetext_id).replace('files/', '')\n",
    "#         print(segment_file)\n",
    "        with open(segment_file, 'r', encoding=\"cp1251\") as file:\n",
    "            tt = file.read()\n",
    "        \n",
    "        boundaries = set()\n",
    "        mm = [i for i in tt.split('\\n') if len(i)>0]\n",
    "        for i in mm:\n",
    "            boundaries.add(eval(i.split()[0]))\n",
    "            boundaries.add(eval(i.split()[1]))\n",
    "        boundaries = sorted(list(boundaries))\n",
    "        \n",
    "        return {'audio_file':os.path.join(self.path, audio_filepath), \n",
    "                'segment_file':segment_file, \n",
    "                'id':filetext_id, 'sample': signal, \n",
    "                'length': len(signal), 'boundaries': boundaries}\n",
    "        \n",
    "#         return {'id':filetext_id, 'sample': signal, 'length': len(signal), 'boundaries': boundaries}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5341890",
   "metadata": {
    "cellId": "qvr0zh4yguera6re8fl1f"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "def collate_fn(samples):\n",
    "    \n",
    "    max_length = max([sample['length'] for sample in samples])\n",
    "    boundaries = [sample['boundaries'] for sample in samples]\n",
    "    samples1 = []\n",
    "    lengths = []\n",
    "    samplings = []\n",
    "    attentions = []\n",
    "    ids = []\n",
    "    audio_files = []\n",
    "    segment_files = []\n",
    "    for sample in samples:\n",
    "        to_add_l = max_length-sample['length']\n",
    "        sample1 = list(sample['sample'])+[0]*to_add_l\n",
    "        samples1.append(torch.Tensor(sample1).unsqueeze(0))\n",
    "        lengths.append(sample['length'])\n",
    "        ids.append(sample['id'])\n",
    "        audio_files.append(sample['audio_file'])\n",
    "        segment_files.append(sample['segment_file'])\n",
    "        att_norm = torch.ones(size = (1, sample['length']))\n",
    "        att_add = torch.zeros(size = (1, to_add_l))\n",
    "        att = torch.cat([att_norm, att_add], dim = -1)\n",
    "        attentions.append(att)\n",
    "        \n",
    "    batch = torch.cat(samples1)\n",
    "    lengths = torch.Tensor(lengths)\n",
    "    attention_mask = torch.cat(attentions, dim = 0)\n",
    "    \n",
    "    return dict(batch=batch, lengths=lengths, attention_mask=attention_mask, \n",
    "                boundaries=boundaries, ids=ids, \n",
    "                audio_files=audio_files, \n",
    "                segment_files=segment_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa10a4df",
   "metadata": {
    "cellId": "si84mcfqxf9vw88jq4dnx"
   },
   "source": [
    "# Predict Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "295e8da8",
   "metadata": {
    "cellId": "6c5xz7m4h1rc8i4fugle0c"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "train_dataset = Dataset('train/', 'segments_edges_init/', '10hours_train.json', \n",
    "                        edges_path = 'silero_edges',\n",
    "                              chars = 'segments_chars/', frames = 'secs/')\n",
    "val_dataset = Dataset('train/', 'segments_edges_init/', '10hours_val.json', \n",
    "                        edges_path = 'silero_edges',\n",
    "                              chars = 'segments_chars/', frames = 'secs/')\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=8, collate_fn = collate_fn)\n",
    "val_loader = DataLoader(val_dataset, shuffle=False, batch_size=8, collate_fn = collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1578442",
   "metadata": {
    "cellId": "n5459u4c7o7dnc4dvw9w6q"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3680a2cb",
   "metadata": {
    "cellId": "pckte08idi79u7de4gz33"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ed4d16a",
   "metadata": {
    "cellId": "68f7eeo0o1ym3uvq8q9jxe"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "path_to_save = 'save_results_path_10_hours_conv'\n",
    "path_results = os.path.join(path_to_save, 'Results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44b3595b",
   "metadata": {
    "cellId": "k6bl8ogekrvces717m5tt"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "os.makedirs(path_to_save, exist_ok=True) \n",
    "os.makedirs(path_results, exist_ok=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0cd4cff",
   "metadata": {
    "cellId": "r8ivrkng5kmmni9ftvr9kp"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Audio', 'Segment', 'Results']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "os.listdir(path_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bfa87a58",
   "metadata": {
    "cellId": "geneu8n429sdabta8o7zug"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b883f2a7",
   "metadata": {
    "cellId": "rr245v1euian3igs7zwol"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "for batch in val_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80d833de",
   "metadata": {
    "cellId": "ilh62n15hglj2cfelks4k"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6525070",
   "metadata": {
    "cellId": "5f7wyndbo4nxgj1pw3tiod"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from transformers_f.src.transformers.activations import ACT2FN\n",
    "from transformers_f.src.transformers.models.wav2vec2.modeling_wav2vec2 import Wav2Vec2FeatureExtractor, Wav2Vec2Model\n",
    "\n",
    "from transformers_f.src.transformers.models.wav2vec2.modeling_segmentation import (Wav2Vec2ModelForSegmentation,\n",
    "                                                                                   SegmentsRepr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1f72c73",
   "metadata": {
    "cellId": "sdpxmprodbi5f7rbttx8dd"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "segment_paths = ['golos_model_segment_r_val_acc_200_edges_train_10_hours.ckpt']\n",
    "names_paths = ['edges']\n",
    "thres = [0.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5ade274",
   "metadata": {
    "cellId": "ba7l71rq11crxe5hojcjyo"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eef17320",
   "metadata": {
    "cellId": "guaglf5zdun15mdnnr65z7"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "accumulate_grad_batches = 10\n",
    "cfg = {'optimizer': \"adam\",\n",
    "'momentum': 0.9,\n",
    "'learning_rate': 0.0001*accumulate_grad_batches,\n",
    "'lr_anneal_gamma': 1.0,\n",
    "'lr_anneal_step': 1000,\n",
    "# 'epochs': 500,\n",
    "'grad_clip': 0.5,\n",
    "'batch_size': 8,\n",
    "\n",
    "'conv_args': {},\n",
    "'mask_args': {},\n",
    "'segm_enc_args': {},\n",
    "'segm_predictor_args': {},\n",
    "'loss_args': {\"n_negatives\": 10, \"loss_args\": {\"reduction\": \"mean\"}},\n",
    "'num_epoch': 2}\n",
    "\n",
    "class Conf:\n",
    "    def __init__(self, my_dict):\n",
    "        for key, value in my_dict.items():\n",
    "            setattr(self, key, value)\n",
    "            \n",
    "config = Conf(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d0a51b1",
   "metadata": {
    "cellId": "5yetigc4musoftlbk4y6l"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "95f64cf7",
   "metadata": {
    "cellId": "dc6jpndd8mfco3se4w9in5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1050/1050 [04:09<00:00,  4.20it/s]\n",
      "100%|██████████| 63/63 [00:13<00:00,  4.60it/s]\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "for model_path, name_path, thre in zip(segment_paths, names_paths, thres):\n",
    "    os.makedirs(os.path.join(path_results, name_path), exist_ok=True) \n",
    "    \n",
    "    wav2vec_segm = FinModel(config)\n",
    "    \n",
    "    checkpoint = torch.load(model_path)\n",
    "#     checkpoint = torch.load(model_path, map_location='cpu')\n",
    "    \n",
    "    state_dicts = OrderedDict()\n",
    "    for key, value in checkpoint['state_dict'].items():\n",
    "        state_dicts[key.replace('wav2vec_segm.', '')] = value\n",
    "    wav2vec_segm.load_state_dict(state_dicts)\n",
    "    \n",
    "    wav2vec_segm.eval()\n",
    "    wav2vec_segm=wav2vec_segm.to('cuda')\n",
    "    \n",
    "    for batch in tqdm(train_loader):\n",
    "        x = batch['batch']\n",
    "        lengths = batch['lengths']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        secs = batch['boundaries']\n",
    "        ids = batch['ids']\n",
    "        name_path = 'train'\n",
    "        os.makedirs(os.path.join(path_results, name_path), exist_ok=True) \n",
    "\n",
    "        rr = wav2vec_segm.compute_all(x.to('cuda'), secs, num_epoch=0, attention_mask=attention_mask.to('cuda'), return_secs=True)\n",
    "#         rr = wav2vec_segm.compute_all(x, secs, num_epoch=0, attention_mask=attention_mask, return_secs=True)\n",
    "        secs_preds = rr[1]['secs_pred']\n",
    "        for idd, secs in zip(ids, secs_preds):\n",
    "            with open(os.path.join(os.path.join(path_results, name_path), idd), 'w', encoding=\"cp1251\") as file:\n",
    "                file.write(str(secs))\n",
    "\n",
    "    for batch in tqdm(val_loader):\n",
    "        x = batch['batch']\n",
    "        lengths = batch['lengths']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        secs = batch['boundaries']\n",
    "        ids = batch['ids']\n",
    "        name_path = 'val'\n",
    "        os.makedirs(os.path.join(path_results, name_path), exist_ok=True) \n",
    "\n",
    "        rr = wav2vec_segm.compute_all(x.to('cuda'), secs, num_epoch=0, attention_mask=attention_mask.to('cuda'), return_secs=True)\n",
    "#         rr = wav2vec_segm.compute_all(x, secs, num_epoch=0, attention_mask=attention_mask, return_secs=True)\n",
    "        secs_preds = rr[1]['secs_pred']\n",
    "        for idd, secs in zip(ids, secs_preds):\n",
    "            with open(os.path.join(os.path.join(path_results, name_path), idd), 'w', encoding=\"cp1251\") as file:\n",
    "                file.write(str(secs))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a2de9199",
   "metadata": {
    "cellId": "1uoazf7nvu72dagphnr19w"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6079bdaf",
   "metadata": {
    "cellId": "p9fg5n0gzrock6y36ewspo"
   },
   "source": [
    "# Predict Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe0bcad1",
   "metadata": {
    "cellId": "m260cbjx5xquckua910pc"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "class Dataset_test:\n",
    "    \n",
    "    def __init__(self, path, segment_path, manifest_path, edges_path = None, chars = 'segments_chars/', frames = 'secs/'):\n",
    "        with open(manifest_path, 'r') as json_file:\n",
    "            manifest = json.load(json_file)\n",
    "        self.manifest = manifest\n",
    "#         self.manifest = manifest[:1000]\n",
    "        self.path = path\n",
    "        self.segment_path = segment_path\n",
    "        self.frames = os.path.join(chars, frames)\n",
    "        self.edges_path = edges_path\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.manifest)\n",
    "    \n",
    "    def __getitem__(self, ind):\n",
    "#         print(self.manifest[ind])\n",
    "        \n",
    "        # Загрузка аудио-сигнала\n",
    "        \n",
    "        audio_filepath = self.manifest[ind]['audio_filepath']\n",
    "#         print(audio_filepath)\n",
    "        audio_file = os.path.join(self.path, audio_filepath)\n",
    "#         print(audio_file)\n",
    "        sampling_rate, signal = wav.read(audio_file)\n",
    "#         signal, sampling_rate = torchaudio.load(path)\n",
    "        \n",
    "        # Обрезать тишину\n",
    "        filetext_id = self.manifest[ind]['id']+'.txt'\n",
    "        silero_filepath = audio_filepath.replace('.wav', '.txt')\n",
    "        silero_file = os.path.join(self.edges_path, silero_filepath).replace('files/', '')\n",
    "#         print(silero_file)\n",
    "        \n",
    "        with open(silero_file, 'r', encoding=\"cp1251\") as file:\n",
    "            tt = file.read()\n",
    "        \n",
    "        start = []\n",
    "        end = []\n",
    "        for line in tt.split('\\n'):\n",
    "            if len(line) > 0:\n",
    "                start.append(eval(line.split()[0]))\n",
    "                end.append(eval(line.split()[1]))\n",
    "        start = min(start)\n",
    "        end = max(end)\n",
    "#         print(start)\n",
    "#         print(end)\n",
    "        signal = signal[start:end]\n",
    "        \n",
    "        # Загрузка разметки\n",
    "        filetext_id = self.manifest[ind]['id']+'.txt'\n",
    "        segment_filepath = audio_filepath.replace('.wav', '.txt').replace(filetext_id, '')\n",
    "        \n",
    "        segment_file = os.path.join(os.path.join(os.path.join(self.segment_path, segment_filepath), self.frames), filetext_id).replace('files/', '')\n",
    "#         print(segment_file)\n",
    "        with open(segment_file, 'r', encoding=\"cp1251\") as file:\n",
    "            tt = file.read()\n",
    "        \n",
    "        boundaries = set()\n",
    "        mm = [i for i in tt.split('\\n') if len(i)>0]\n",
    "        for i in mm:\n",
    "            boundaries.add(eval(i.split()[0]))\n",
    "            boundaries.add(eval(i.split()[1]))\n",
    "        boundaries = sorted(list(boundaries))\n",
    "        \n",
    "        return {'audio_file':os.path.join(self.path, audio_filepath), \n",
    "                'segment_file':segment_file, \n",
    "                'id':filetext_id, 'sample': signal, \n",
    "                'length': len(signal), 'boundaries': boundaries}\n",
    "        \n",
    "#         return {'id':filetext_id, 'sample': signal, 'length': len(signal), 'boundaries': boundaries}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "72fa8123",
   "metadata": {
    "cellId": "rulah2g4vbh0qi5oiyz3g"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "test_farfield_dataset = Dataset_test('test/', 'segments_edges_init_test/', 'manifest_test_farfield.json', \n",
    "                        edges_path = 'silero_edges_test',\n",
    "                              chars = 'segments_chars/', frames = 'secs/')\n",
    "test_crowd_dataset = Dataset_test('test/', 'segments_edges_init_test/', 'manifest_test_crowd.json', \n",
    "                        edges_path = 'silero_edges_test',\n",
    "                              chars = 'segments_chars/', frames = 'secs/')\n",
    "\n",
    "test_farfield_loader = DataLoader(test_farfield_dataset, shuffle=True, batch_size=8, collate_fn = collate_fn)\n",
    "test_crowd_loader = DataLoader(test_crowd_dataset, shuffle=True, batch_size=8, collate_fn = collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d02ada8d",
   "metadata": {
    "cellId": "ae2a60e3tmd4rz6kw1ugel"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e58eb50b",
   "metadata": {
    "cellId": "if2n83cgrgeal40wg2w1e"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "86a3645a",
   "metadata": {
    "cellId": "oq4sjb4ooklcen7bsngbua"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from transformers_f.src.transformers.activations import ACT2FN\n",
    "from transformers_f.src.transformers.models.wav2vec2.modeling_wav2vec2 import Wav2Vec2FeatureExtractor, Wav2Vec2Model\n",
    "\n",
    "from transformers_f.src.transformers.models.wav2vec2.modeling_segmentation import (Wav2Vec2ModelForSegmentation,\n",
    "                                                                                   SegmentsRepr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a9e564db",
   "metadata": {
    "cellId": "tr8h06zm0lfu9lqokq0ly"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "for batch in test_farfield_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "db7b050b",
   "metadata": {
    "cellId": "eev918xpi9vtz9ay6n8zlf"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "57b4c066",
   "metadata": {
    "cellId": "r5zpolo0edonqewwpxjeo"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "\n",
    "segment_paths = ['golos_model_segment_r_val_acc_200_edges_train_10_hours.ckpt']\n",
    "names_paths = ['edges']\n",
    "thres = [0.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3986c9b0",
   "metadata": {
    "cellId": "u4kzeaxce7irp9v36bpwj"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "path_to_save = 'save_results_path_10_hours_conv'\n",
    "path_results = os.path.join(path_to_save, 'Results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c44dc6b7",
   "metadata": {
    "cellId": "avw5bdcb2dc8tujy9huzxn"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "os.makedirs(path_to_save, exist_ok=True) \n",
    "os.makedirs(path_results, exist_ok=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ce80feea",
   "metadata": {
    "cellId": "ekhpo4pu0z6jaledvwllip"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ac360a76",
   "metadata": {
    "cellId": "5ln90gb01na8c7rkovk944"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "accumulate_grad_batches = 10\n",
    "cfg = {'optimizer': \"adam\",\n",
    "'momentum': 0.9,\n",
    "'learning_rate': 0.0001*accumulate_grad_batches,\n",
    "'lr_anneal_gamma': 1.0,\n",
    "'lr_anneal_step': 1000,\n",
    "# 'epochs': 500,\n",
    "'grad_clip': 0.5,\n",
    "'batch_size': 8,\n",
    "\n",
    "'conv_args': {},\n",
    "'mask_args': {},\n",
    "'segm_enc_args': {},\n",
    "'segm_predictor_args': {},\n",
    "'loss_args': {\"n_negatives\": 10, \"loss_args\": {\"reduction\": \"mean\"}},\n",
    "'num_epoch': 2}\n",
    "\n",
    "class Conf:\n",
    "    def __init__(self, my_dict):\n",
    "        for key, value in my_dict.items():\n",
    "            setattr(self, key, value)\n",
    "            \n",
    "config = Conf(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0a2c66de",
   "metadata": {
    "cellId": "xn1rtcra2njs3x4kfuimb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 240/240 [00:46<00:00,  5.19it/s]\n",
      "100%|██████████| 1237/1237 [04:14<00:00,  4.87it/s]\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "for model_path, name_path, thre in zip(segment_paths, names_paths, thres):\n",
    "    os.makedirs(os.path.join(path_results, name_path), exist_ok=True) \n",
    "    \n",
    "    wav2vec_segm = FinModel(config)\n",
    "    \n",
    "    checkpoint = torch.load(model_path)\n",
    "#     checkpoint = torch.load(model_path, map_location='cpu')\n",
    "    \n",
    "    state_dicts = OrderedDict()\n",
    "    for key, value in checkpoint['state_dict'].items():\n",
    "        state_dicts[key.replace('wav2vec_segm.', '')] = value\n",
    "    wav2vec_segm.load_state_dict(state_dicts)\n",
    "    \n",
    "    wav2vec_segm.eval()\n",
    "    wav2vec_segm=wav2vec_segm.to('cuda')\n",
    "    \n",
    "    for batch in tqdm(test_farfield_loader):\n",
    "        x = batch['batch']\n",
    "        lengths = batch['lengths']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        secs = batch['boundaries']\n",
    "        ids = batch['ids']\n",
    "        name_path = 'farfield'\n",
    "        os.makedirs(os.path.join(path_results, name_path), exist_ok=True) \n",
    "\n",
    "        rr = wav2vec_segm.compute_all(x.to('cuda'), secs, num_epoch=0, attention_mask=attention_mask.to('cuda'), return_secs=True)\n",
    "#         rr = wav2vec_segm.compute_all(x, secs, num_epoch=0, attention_mask=attention_mask, return_secs=True)\n",
    "        secs_preds = rr[1]['secs_pred']\n",
    "        for idd, secs in zip(ids, secs_preds):\n",
    "            with open(os.path.join(os.path.join(path_results, name_path), idd), 'w', encoding=\"cp1251\") as file:\n",
    "                file.write(str(secs))\n",
    "                \n",
    "    for batch in tqdm(test_crowd_loader):\n",
    "        x = batch['batch']\n",
    "        lengths = batch['lengths']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        secs = batch['boundaries']\n",
    "        ids = batch['ids']\n",
    "        name_path = 'crowd'\n",
    "        os.makedirs(os.path.join(path_results, name_path), exist_ok=True) \n",
    "\n",
    "        rr = wav2vec_segm.compute_all(x.to('cuda'), secs, num_epoch=0, attention_mask=attention_mask.to('cuda'), return_secs=True)\n",
    "#         rr = wav2vec_segm.compute_all(x, secs, num_epoch=0, attention_mask=attention_mask, return_secs=True)\n",
    "        secs_preds = rr[1]['secs_pred']\n",
    "        for idd, secs in zip(ids, secs_preds):\n",
    "            with open(os.path.join(os.path.join(path_results, name_path), idd), 'w', encoding=\"cp1251\") as file:\n",
    "                file.write(str(secs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc1ae55",
   "metadata": {
    "cellId": "nbtj4lbdfxrcxbhlr4or7"
   },
   "source": [
    "# Get results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "81dacf7f",
   "metadata": {
    "cellId": "bokcdvz728ev45v0bj4kjd"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2dc7c4b1",
   "metadata": {
    "cellId": "pip9xt7km4f9rg1ycbhyat"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "class RMetrics1(nn.Module):\n",
    "    def __init__(self, eps = 1e-5, tolerance = 2, sampling_rate = 16000):\n",
    "        super(RMetrics1, self).__init__()\n",
    "        self.tolerance = tolerance\n",
    "        self.eps = eps\n",
    "        self.sampling_rate = sampling_rate\n",
    "    \n",
    "    def calculate_stride(self, isz, conv_layers):\n",
    "        pad = 0\n",
    "        insize = isz\n",
    "        totstride = 1\n",
    "        sec_per_frame = 1/self.sampling_rate\n",
    "\n",
    "        for layer in conv_layers:\n",
    "            kernel, stride = layer\n",
    "            outsize = (insize + 2*pad - 1*(kernel-1)-1) / stride + 1\n",
    "            insize = outsize\n",
    "            totstride = totstride * stride\n",
    "\n",
    "        RFsize = isz - (outsize - 1) * totstride\n",
    "\n",
    "        ms_per_frame = sec_per_frame*RFsize*1000\n",
    "        ms_stride = sec_per_frame*totstride*1000\n",
    "        return outsize, totstride, RFsize, ms_per_frame, ms_stride\n",
    "        \n",
    "    def get_frames(self, secs, stride):\n",
    "        frames = [[int(i*self.sampling_rate/stride) for i in sec] for sec in secs]\n",
    "        return frames\n",
    "        \n",
    "    def make_true_boundaries(self, secs, boundaries, stride):\n",
    "        frames = self.get_frames(secs, stride)\n",
    "        true_boundaries = torch.zeros(size = boundaries.shape)\n",
    "        for num_frame, frame in enumerate(frames):\n",
    "            for i in frame:\n",
    "                true_boundaries[num_frame, i] = 1\n",
    "        return true_boundaries.long().detach().numpy()\n",
    "    \n",
    "    def get_sec_bounds(self, b, stride, attention_mask = None):\n",
    "        if type(b)==torch.Tensor:\n",
    "            b1 = b.long().detach().cpu().numpy()\n",
    "        else:\n",
    "            b1 = b\n",
    "        \n",
    "        if attention_mask is not None:\n",
    "            b1 = b1*attention_mask.long().detach().cpu().numpy()\n",
    "            \n",
    "        frames_pred = []\n",
    "        secs_pred = []\n",
    "        for i in range(b1.shape[0]):\n",
    "            frames = np.where(b1[i, :] == 1)[0]\n",
    "            secs = [i*stride/self.sampling_rate for i in frames]\n",
    "            frames_pred.append(frames)\n",
    "            secs_pred.append(secs)\n",
    "        return frames_pred, secs_pred\n",
    "    \n",
    "    def get_precision_recall_frames(self, true_boundaries, b, attention_mask = None):\n",
    "        if type(b)==torch.Tensor:\n",
    "            b1 = b.long().detach().numpy()\n",
    "        else:\n",
    "            b1 = b\n",
    "            \n",
    "        if attention_mask is not None:\n",
    "            b1 = b1*attention_mask.long().detach().cpu().numpy()\n",
    "            \n",
    "        recall = recall_score(true_boundaries.flatten(), b1.flatten())\n",
    "        pre = precision_score(true_boundaries.flatten(), b1.flatten())\n",
    "        f_score = f1_score(true_boundaries.flatten(), b1.flatten())\n",
    "        return recall, pre, f_score\n",
    "    \n",
    "    def get_stats(self, frames_true, frames_pred):\n",
    "        \n",
    "        # Утащено отсюда: https://github.com/felixkreuk/UnsupSeg/blob/68c2c7b9bd49f3fb8f51c5c2f4d5aa85f251eaa8/utils.py#L69\n",
    "        precision_counter = 0 \n",
    "        recall_counter = 0\n",
    "        pred_counter = 0 \n",
    "        gt_counter = 0\n",
    "\n",
    "        for (y, yhat) in zip(frames_true, frames_pred):\n",
    "            for yhat_i in yhat:\n",
    "                min_dist = np.abs(np.array(y) - yhat_i).min()\n",
    "                precision_counter += (min_dist <= self.tolerance)\n",
    "            for y_i in y:\n",
    "                min_dist = np.abs(np.array(yhat) - y_i).min()\n",
    "                recall_counter += (min_dist <= self.tolerance)\n",
    "            pred_counter += len(yhat)\n",
    "            gt_counter += len(y)\n",
    "\n",
    "        return precision_counter, recall_counter, pred_counter, gt_counter\n",
    "    \n",
    "    def calc_metr(self, precision_counter, recall_counter, pred_counter, gt_counter):\n",
    "\n",
    "        # Утащено отсюда: https://github.com/felixkreuk/UnsupSeg/blob/68c2c7b9bd49f3fb8f51c5c2f4d5aa85f251eaa8/utils.py#L69\n",
    "        EPS = 1e-7\n",
    "\n",
    "        precision = precision_counter / (pred_counter + self.eps)\n",
    "        recall = recall_counter / (gt_counter + self.eps)\n",
    "        f1 = 2 * (precision * recall) / (precision + recall + self.eps)\n",
    "\n",
    "        os = recall / (precision + EPS) - 1\n",
    "        r1 = np.sqrt((1 - recall) ** 2 + os ** 2)\n",
    "        r2 = (-os + recall - 1) / (np.sqrt(2))\n",
    "        rval = 1 - (np.abs(r1) + np.abs(r2)) / 2\n",
    "\n",
    "        return precision, recall, f1, rval\n",
    "    \n",
    "    def get_metrics(self, true_secs, b, seq_len, config, attention_mask = None, \n",
    "                    return_secs=False):\n",
    "        \n",
    "        outsize, totstride, RFsize, ms_per_frame, ms_stride = self.calculate_stride(seq_len, config)\n",
    "#         print(seq_len, outsize, totstride, RFsize, ms_per_frame, ms_stride)\n",
    "        frames_true = self.get_frames(true_secs, totstride)\n",
    "        frames_pred, secs_pred = self.get_sec_bounds(b, totstride, attention_mask = attention_mask)\n",
    "        precision_counter, recall_counter, pred_counter, gt_counter = self.get_stats(frames_true, frames_pred)\n",
    "        precision, recall, f1, rval = self.calc_metr(precision_counter, recall_counter, pred_counter, gt_counter)\n",
    "        if return_secs:\n",
    "            return precision, recall, f1, rval, secs_pred\n",
    "        else:\n",
    "            return precision, recall, f1, rval\n",
    "        \n",
    "    def get_metrics_secs(self, true_secs, secs_pred, totstride = 160):\n",
    "        \n",
    "        frames_true = self.get_frames(true_secs, totstride)\n",
    "        frames_pred = self.get_frames(secs_pred, totstride)\n",
    "        precision_counter, recall_counter, pred_counter, gt_counter = self.get_stats(frames_true, frames_pred)\n",
    "        precision, recall, f1, rval = self.calc_metr(precision_counter, recall_counter, pred_counter, gt_counter)\n",
    "        return precision, recall, f1, rval\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ab465b5c",
   "metadata": {
    "cellId": "vhtwznk2mbjwspm649bpq"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "def read_true_file(segment_file):\n",
    "    with open(segment_file, 'r', encoding=\"cp1251\") as file:\n",
    "        tt = file.read()\n",
    "\n",
    "    boundaries = set()\n",
    "    mm = [i for i in tt.split('\\n') if len(i)>0]\n",
    "    for i in mm:\n",
    "        boundaries.add(eval(i.split()[0]))\n",
    "        boundaries.add(eval(i.split()[1]))\n",
    "    boundaries = sorted(list(boundaries))\n",
    "    return boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4cf81c06",
   "metadata": {
    "cellId": "40kryrbbjnkedurh7viv5n"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "def read_pred_file(segment_file):\n",
    "    with open(segment_file, 'r', encoding=\"cp1251\") as file:\n",
    "        tt = file.read()\n",
    "    boundaries = eval(tt)\n",
    "    if len(boundaries)>1:\n",
    "        return boundaries[1:]\n",
    "    else:\n",
    "        return boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f5b3dd35",
   "metadata": {
    "cellId": "snzs3mssc1hqjdilocb2"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "class Dataset_res:\n",
    "    \n",
    "    def __init__(self, path, segment_path, manifest_path, chars = 'segments_chars/', frames = 'secs/'):\n",
    "        with open(manifest_path, 'r') as json_file:\n",
    "            manifest = json.load(json_file)\n",
    "        self.manifest = manifest\n",
    "        self.path = path\n",
    "        self.segment_path = segment_path\n",
    "        self.frames = os.path.join(chars, frames)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.manifest)\n",
    "    \n",
    "    def __getitem__(self, ind):\n",
    "#         print(self.manifest[ind])\n",
    "        \n",
    "        # Загрузка аудио-сигнала\n",
    "        \n",
    "        audio_filepath = self.manifest[ind]['audio_filepath']\n",
    "#         print(audio_filepath)\n",
    "        audio_file = os.path.join(self.path, audio_filepath)\n",
    "    \n",
    "        filetext_id = self.manifest[ind]['id']+'.txt'\n",
    "        segment_filepath = audio_filepath.replace('.wav', '.txt').replace(filetext_id, '')\n",
    "        \n",
    "        segment_file = os.path.join(os.path.join(os.path.join(self.segment_path, segment_filepath), self.frames), filetext_id).replace('files/', '')\n",
    "#         print(segment_file)\n",
    "        with open(segment_file, 'r', encoding=\"cp1251\") as file:\n",
    "            tt = file.read()\n",
    "        \n",
    "        boundaries = set()\n",
    "        mm = [i for i in tt.split('\\n') if len(i)>0]\n",
    "        for i in mm:\n",
    "            boundaries.add(eval(i.split()[0]))\n",
    "            boundaries.add(eval(i.split()[1]))\n",
    "        boundaries = sorted(list(boundaries))\n",
    "        \n",
    "        return {'segment_file':segment_file, \n",
    "                'id':filetext_id,\n",
    "                'boundaries': boundaries}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "782ef575",
   "metadata": {
    "cellId": "leaa0dnmu7f0hlrmmwn2mgu"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "def collate_fn_res(samples):\n",
    "\n",
    "    boundaries = [sample['boundaries'] for sample in samples]\n",
    "    samples1 = []\n",
    "    lengths = []\n",
    "    samplings = []\n",
    "    attentions = []\n",
    "    ids = []\n",
    "    audio_files = []\n",
    "    segment_files = []\n",
    "    for sample in samples:\n",
    "        ids.append(sample['id'])\n",
    "        audio_files.append(sample['audio_file'])\n",
    "        segment_files.append(sample['segment_file'])\n",
    "        \n",
    "    \n",
    "    return dict(batch=batch, lengths=lengths, \n",
    "                boundaries=boundaries, ids=ids, \n",
    "                audio_files=audio_files, \n",
    "                segment_files=segment_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3af8e958",
   "metadata": {
    "cellId": "nry85isbeiodn1s7sal4io"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "train_dataset = Dataset('train/', 'segments_edges_init/', '10hours_train.json', \n",
    "                        edges_path = 'silero_edges',\n",
    "                              chars = 'segments_chars/', frames = 'secs/')\n",
    "val_dataset = Dataset('train/', 'segments_edges_init/', '10hours_val.json', \n",
    "                        edges_path = 'silero_edges',\n",
    "                              chars = 'segments_chars/', frames = 'secs/')\n",
    "\n",
    "# test_dataset = Dataset_res('test/', 'segments_edges_init_test/', 'manifest_test.json', \n",
    "#                               chars = 'segments_chars/', frames = 'secs/')\n",
    "test_farfield_dataset = Dataset_test('test/', 'segments_edges_init_test/', 'manifest_test_farfield.json', \n",
    "                        edges_path = 'silero_edges_test',\n",
    "                              chars = 'segments_chars/', frames = 'secs/')\n",
    "test_crowd_dataset = Dataset_test('test/', 'segments_edges_init_test/', 'manifest_test_crowd.json', \n",
    "                        edges_path = 'silero_edges_test',\n",
    "                              chars = 'segments_chars/', frames = 'secs/')\n",
    "\n",
    "test_farfield_loader = DataLoader(test_farfield_dataset, shuffle=True, batch_size=8, collate_fn = collate_fn)\n",
    "test_crowd_loader = DataLoader(test_crowd_dataset, shuffle=True, batch_size=8, collate_fn = collate_fn)\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=8, collate_fn = collate_fn_res)\n",
    "val_loader = DataLoader(val_dataset, shuffle=False, batch_size=8, collate_fn = collate_fn_res)\n",
    "# test_loader = DataLoader(test_dataset, shuffle=True, batch_size=8, collate_fn = collate_fn_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6d87d5f7",
   "metadata": {
    "cellId": "sd4d46qqeljzn00ctrnxhn"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "path_to_save = 'save_results_path_10_hours_conv'\n",
    "path_results = os.path.join(path_to_save, 'Results')\n",
    "folders = ['train', 'val', 'farfield', 'crowd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4543c00d",
   "metadata": {
    "cellId": "c5vgacnrn18pxs50wc5zl"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8398/8398 [00:32<00:00, 257.57it/s]\n",
      "100%|██████████| 500/500 [00:06<00:00, 79.87it/s]\n",
      "100%|██████████| 1915/1915 [00:18<00:00, 102.41it/s]\n",
      "100%|██████████| 9889/9889 [00:53<00:00, 185.97it/s]\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "result_dataframes = []\n",
    "metr = RMetrics1()\n",
    "outsize, totstride, RFsize, ms_per_frame, ms_stride = metr.calculate_stride(100000, \n",
    "                                                                            wav2vec_segm.conv_layers_list)\n",
    "for folder, dataset in zip(folders, [train_dataset, val_dataset, test_farfield_dataset, test_crowd_dataset]):\n",
    "\n",
    "    secs_preds = []\n",
    "    secs_trues = []\n",
    "\n",
    "    for i in tqdm(range(len(dataset))):\n",
    "        \n",
    "        idd = dataset[i]\n",
    "        true_file = idd['segment_file']\n",
    "        bound_true = idd['boundaries']\n",
    "        ids = idd['id']\n",
    "        \n",
    "#         bound_true = read_true_file(true_files[num])\n",
    "        bound_pred = read_pred_file(os.path.join(os.path.join(path_results, folder), ids))\n",
    "\n",
    "        secs_trues.append(bound_true)\n",
    "        secs_preds.append(bound_pred)\n",
    "   \n",
    "\n",
    "    precision, recall, f1, rval = metr.get_metrics_secs(secs_trues, secs_preds, totstride = totstride)\n",
    "    \n",
    "    datafr = pd.DataFrame([folder, \n",
    "                           precision, recall, \n",
    "                           f1, rval]).T.rename(columns = {0:'type', \n",
    "                                                          1:'precision',\n",
    "                                                          2:'recall', \n",
    "                                                          3:'f1', \n",
    "                                                          4:'rval'})\n",
    "    \n",
    "    result_dataframes.append(datafr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5773c26d",
   "metadata": {
    "cellId": "f6zpboo9xwwxojdyu889k"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "result_df = pd.concat(result_dataframes, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "bb5e16e1",
   "metadata": {
    "cellId": "gpg3h3d370d52poqpqkeim"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>rval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train</td>\n",
       "      <td>0.634034</td>\n",
       "      <td>0.64674</td>\n",
       "      <td>0.640319</td>\n",
       "      <td>0.691105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>val</td>\n",
       "      <td>0.627283</td>\n",
       "      <td>0.646142</td>\n",
       "      <td>0.636568</td>\n",
       "      <td>0.686697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>farfield</td>\n",
       "      <td>0.569982</td>\n",
       "      <td>0.688024</td>\n",
       "      <td>0.62346</td>\n",
       "      <td>0.629251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>crowd</td>\n",
       "      <td>0.566495</td>\n",
       "      <td>0.639342</td>\n",
       "      <td>0.600713</td>\n",
       "      <td>0.635576</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       type precision    recall        f1      rval\n",
       "0     train  0.634034   0.64674  0.640319  0.691105\n",
       "1       val  0.627283  0.646142  0.636568  0.686697\n",
       "2  farfield  0.569982  0.688024   0.62346  0.629251\n",
       "3     crowd  0.566495  0.639342  0.600713  0.635576"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9d573d9c",
   "metadata": {
    "cellId": "2oxvkbximvofrvan3dx5a"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "result_df.to_csv('results_golos_sep_test_conv1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b4a759",
   "metadata": {
    "cellId": "0yj4uzwq5yyjemlq99e6rvr"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Yandex DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "notebookId": "3bc9a48a-b0c2-40e8-af54-3233d9b054ee",
  "notebookPath": "Predict_10_hours_conv.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
