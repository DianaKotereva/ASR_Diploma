{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b62972e",
   "metadata": {
    "cellId": "g09276vaso83hmt0w831sn"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import scipy.io.wavfile as wav\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import numpy as np\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import math\n",
    "\n",
    "import torchaudio\n",
    "from boltons.fileutils import iter_find_files\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0d4dc34",
   "metadata": {
    "cellId": "uvl72wmlsin2pm85am363"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "# %pip install pytorch_lightning --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56980a5f",
   "metadata": {
    "cellId": "orkvm83jfktjymx3w6hs"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from models import ConvFeatureEncoder, SegmentsRepr, SegmentsEncoder, NegativeSampler, SegmentPredictor, FinModel, FinModel1\n",
    "from utils import ConstrativeLoss, sample_negatives\n",
    "# from trainer import Trainer\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f551f07",
   "metadata": {
    "cellId": "gz19l6aiug6sr9hmk0laq"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "# import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d37439e2",
   "metadata": {
    "cellId": "nqde74hmycnt2bja1wje"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4021bcc7",
   "metadata": {
    "cellId": "dd60n8qgnt17m37fafvrd"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "def spectral_size(wav_len):\n",
    "    layers = [(10,5,0), (8,4,0), (4,2,0), (4,2,0), (4,2,0)]\n",
    "    for kernel, stride, padding in layers:\n",
    "        wav_len = math.floor((wav_len + 2*padding - 1*(kernel-1) - 1)/stride + 1)\n",
    "    return wav_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50eda300",
   "metadata": {
    "cellId": "p83k0vjdjnagaeirjde2vc"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "class Dataset_test:\n",
    "    \n",
    "    def __init__(self, path, segment_path, manifest_path, edges_path = None, chars = 'segments_chars/', frames = 'secs/'):\n",
    "        with open(manifest_path, 'r') as json_file:\n",
    "            manifest = json.load(json_file)\n",
    "        self.manifest = manifest\n",
    "#         self.manifest = manifest[:1000]\n",
    "        self.path = path\n",
    "        self.segment_path = segment_path\n",
    "        self.frames = os.path.join(chars, frames)\n",
    "        self.edges_path = edges_path\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.manifest)\n",
    "    \n",
    "    \n",
    "    def spectral_size(self, wav_len):\n",
    "        layers = [(10,5,0), (8,4,0), (4,2,0), (4,2,0), (4,2,0)]\n",
    "        for kernel, stride, padding in layers:\n",
    "            wav_len = math.floor((wav_len + 2*padding - 1*(kernel-1) - 1)/stride + 1)\n",
    "        return wav_len\n",
    "    \n",
    "    def __getitem__(self, ind):\n",
    "#         print(self.manifest[ind])\n",
    "        \n",
    "        # Загрузка аудио-сигнала\n",
    "        \n",
    "        audio_filepath = self.manifest[ind]['audio_filepath']\n",
    "#         print(audio_filepath)\n",
    "        audio_file = os.path.join(self.path, audio_filepath)\n",
    "#         print(audio_file)\n",
    "        sampling_rate, signal = wav.read(audio_file)\n",
    "#         signal, sampling_rate = torchaudio.load(path)\n",
    "        \n",
    "        # Обрезать тишину\n",
    "        filetext_id = self.manifest[ind]['id']+'.txt'\n",
    "        silero_filepath = audio_filepath.replace('.wav', '.txt')\n",
    "        silero_file = os.path.join(self.edges_path, silero_filepath).replace('files/', '')\n",
    "#         print(silero_file)\n",
    "        \n",
    "        with open(silero_file, 'r', encoding=\"cp1251\") as file:\n",
    "            tt = file.read()\n",
    "        \n",
    "        start = []\n",
    "        end = []\n",
    "        for line in tt.split('\\n'):\n",
    "            if len(line) > 0:\n",
    "                start.append(eval(line.split()[0]))\n",
    "                end.append(eval(line.split()[1]))\n",
    "        start = min(start)\n",
    "        end = max(end)\n",
    "#         print(start)\n",
    "#         print(end)\n",
    "        signal = signal[start:end]\n",
    "        \n",
    "        # Загрузка разметки\n",
    "        filetext_id = self.manifest[ind]['id']+'.txt'\n",
    "        segment_filepath = audio_filepath.replace('.wav', '.txt').replace(filetext_id, '')\n",
    "        \n",
    "        segment_file = os.path.join(os.path.join(os.path.join(self.segment_path, segment_filepath), self.frames), filetext_id).replace('files/', '')\n",
    "#         print(segment_file)\n",
    "        with open(segment_file, 'r', encoding=\"cp1251\") as file:\n",
    "            tt = file.read()\n",
    "        \n",
    "        boundaries = set()\n",
    "        mm = [i for i in tt.split('\\n') if len(i)>0]\n",
    "        for i in mm:\n",
    "            boundaries.add(eval(i.split()[0]))\n",
    "            boundaries.add(eval(i.split()[1]))\n",
    "        boundaries = sorted(list(boundaries))\n",
    "        \n",
    "        return {'audio_file':os.path.join(self.path, audio_filepath), \n",
    "                'segment_file':segment_file, \n",
    "                'id':filetext_id, 'sample': signal, \n",
    "                'length': len(signal), \n",
    "                'spectral_size': self.spectral_size(len(signal)),\n",
    "                'boundaries': boundaries}\n",
    "        \n",
    "#         return {'id':filetext_id, 'sample': signal, 'length': len(signal), 'boundaries': boundaries}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3547d27a",
   "metadata": {
    "cellId": "s0982jnadwcuboc9726kt"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "def collate_fn(samples):\n",
    "    \n",
    "    max_length = max([sample['length'] for sample in samples])\n",
    "    boundaries = [sample['boundaries'] for sample in samples]\n",
    "    spectral_sizes = [sample['spectral_size'] for sample in samples]\n",
    "    samples1 = []\n",
    "    lengths = []\n",
    "    samplings = []\n",
    "    attentions = []\n",
    "    ids = []\n",
    "    audio_files = []\n",
    "    segment_files = []\n",
    "    for sample in samples:\n",
    "        to_add_l = max_length-sample['length']\n",
    "        sample1 = list(sample['sample'])+[0]*to_add_l\n",
    "        samples1.append(torch.Tensor(sample1).unsqueeze(0))\n",
    "        lengths.append(sample['length'])\n",
    "        ids.append(sample['id'])\n",
    "        audio_files.append(sample['audio_file'])\n",
    "        segment_files.append(sample['segment_file'])\n",
    "        att_norm = torch.ones(size = (1, sample['length']))\n",
    "        att_add = torch.zeros(size = (1, to_add_l))\n",
    "        att = torch.cat([att_norm, att_add], dim = -1)\n",
    "        attentions.append(att)\n",
    "        \n",
    "    batch = torch.cat(samples1)\n",
    "    lengths = torch.Tensor(lengths)\n",
    "    attention_mask = torch.cat(attentions, dim = 0)\n",
    "    spectral_size = torch.Tensor(spectral_sizes)\n",
    "    \n",
    "    return dict(batch=batch, lengths=lengths, attention_mask=attention_mask, \n",
    "                boundaries=boundaries, ids=ids, \n",
    "                audio_files=audio_files, \n",
    "                segment_files=segment_files, \n",
    "                spectral_size=spectral_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201d113f",
   "metadata": {
    "cellId": "l2qzrpxtx1b692ra8i1tsw"
   },
   "source": [
    "# Загрузка Данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "774a9a87",
   "metadata": {
    "cellId": "y8vvvbdy9alvbgpby96ezg"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "test_farfield_dataset = Dataset_test('test/', 'segments_edges_init_test/', 'manifest_test_farfield.json', \n",
    "                        edges_path = 'silero_edges_test',\n",
    "                              chars = 'segments_chars/', frames = 'secs/')\n",
    "test_crowd_dataset = Dataset_test('test/', 'segments_edges_init_test/', 'manifest_test_crowd.json', \n",
    "                        edges_path = 'silero_edges_test',\n",
    "                              chars = 'segments_chars/', frames = 'secs/')\n",
    "test_dataset = Dataset_test('test/', 'segments_edges_init_test/', 'manifest_test.json', \n",
    "                        edges_path = 'silero_edges_test',\n",
    "                              chars = 'segments_chars/', frames = 'secs/')\n",
    "\n",
    "test_farfield_loader = DataLoader(test_farfield_dataset, shuffle=True, batch_size=8, collate_fn = collate_fn)\n",
    "test_crowd_loader = DataLoader(test_crowd_dataset, shuffle=True, batch_size=8, collate_fn = collate_fn)\n",
    "test_loader = DataLoader(test_dataset, shuffle=True, batch_size=8, collate_fn = collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6adf79ce",
   "metadata": {
    "cellId": "5u59w3bje15bpw1bkpjrmi"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "path_results = 'save_results_path_compare_golos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30d6f398",
   "metadata": {
    "cellId": "ifwhoyak8bm2a5g59q2882"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd04d627",
   "metadata": {
    "cellId": "xd4cw7j86gp2yxpt55xdzg"
   },
   "source": [
    "# Segment Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "575a3d7f",
   "metadata": {
    "cellId": "wv9ey7tg0gdosijsccisd"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "model_path = 'golos_model_segment_r_val_acc_200_edges_train_10_hours_model_segment.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1daec8b4",
   "metadata": {
    "cellId": "3xo0einqd098aw9qfu4dgf"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "accumulate_grad_batches = 1\n",
    "cfg = {'optimizer': \"adam\",\n",
    "'momentum': 0.9,\n",
    "'learning_rate': 0.0001*accumulate_grad_batches,\n",
    "'lr_anneal_gamma': 1.0,\n",
    "'lr_anneal_step': 1000,\n",
    "# 'epochs': 500,\n",
    "'grad_clip': 0.5,\n",
    "'batch_size': 8,\n",
    "\n",
    "'conv_args': {},\n",
    "'mask_args': {\"segment\": \"first\", \"add_one\": False},\n",
    "'segm_enc_args': {},\n",
    "'segm_predictor_args': {},\n",
    "'loss_args': {\"n_negatives\": 1, \"loss_args\": {\"reduction\": \"mean\"}},\n",
    "'num_epoch': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3eb87da0",
   "metadata": {
    "cellId": "aldzh2q4pil2dafj0a68h"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "class Conf:\n",
    "    def __init__(self, my_dict):\n",
    "        for key, value in my_dict.items():\n",
    "            setattr(self, key, value)\n",
    "            \n",
    "config = Conf(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d1a6449",
   "metadata": {
    "cellId": "ovym33d2aem1er095sc55h"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 240/240 [00:36<00:00,  6.60it/s]\n",
      "100%|██████████| 1237/1237 [03:23<00:00,  6.08it/s]\n",
      "100%|██████████| 1476/1476 [03:59<00:00,  6.16it/s]\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "model = FinModel(config)\n",
    "checkpoint = torch.load(model_path)\n",
    "#     checkpoint = torch.load(model_path, map_location='cpu')\n",
    "\n",
    "state_dicts = OrderedDict()\n",
    "for key, value in checkpoint['state_dict'].items():\n",
    "    state_dicts[key.replace('wav2vec_segm.', '')] = value\n",
    "model.load_state_dict(state_dicts)\n",
    "\n",
    "model.eval()\n",
    "model=model.to('cuda')\n",
    "name_path = 'model_segment'\n",
    "names = ['farfield', 'crowd', 'all']\n",
    "loaders = [test_farfield_loader, test_crowd_loader, test_loader]\n",
    "fin_path = os.path.join(path_results, name_path)\n",
    "\n",
    "for name, loader in zip(names, loaders):\n",
    "    for batch in tqdm(loader):\n",
    "        x = batch['batch']\n",
    "        lengths = batch['lengths']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        secs = batch['boundaries']\n",
    "        ids = batch['ids']\n",
    "        spectral_sizes = batch['spectral_size']\n",
    "        os.makedirs(os.path.join(fin_path, name), exist_ok=True) \n",
    "\n",
    "\n",
    "        rr = model.compute_all(x.to('cuda'), secs, num_epoch=0, attention_mask=attention_mask.to('cuda'), return_secs=True)\n",
    "    #         rr = model.compute_all(x, secs, num_epoch=0, attention_mask=attention_mask, return_secs=True)\n",
    "        secs_preds = rr[1]['secs_pred']\n",
    "        for idd, secs in zip(ids, secs_preds):\n",
    "            with open(os.path.join(os.path.join(path_results, name_path), idd), 'w', encoding=\"cp1251\") as file:\n",
    "                file.write(str(secs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93ad507f",
   "metadata": {
    "cellId": "iusuzvlxgucelyddt405qt"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b1e9f6",
   "metadata": {
    "cellId": "9vymfx41j1hl6c9eh63nmc"
   },
   "source": [
    "# Peak Detection Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ddb3453a",
   "metadata": {
    "cellId": "ppr15g9556admqq4qhus"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "model_path = 'golos_model_segment_r_val_acc_200_edges_train_10_hours_peak_detection_saved.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4c20852",
   "metadata": {
    "cellId": "fnsu45pn5ynasnyj9rmf8"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "accumulate_grad_batches = 1\n",
    "cfg = {'optimizer': \"adam\",\n",
    "'momentum': 0.9,\n",
    "'learning_rate': 0.0001*accumulate_grad_batches,\n",
    "'lr_anneal_gamma': 1.0,\n",
    "'lr_anneal_step': 1000,\n",
    "# 'epochs': 500,\n",
    "'grad_clip': 0.5,\n",
    "'batch_size': 8,\n",
    "\n",
    "'conv_args': {},\n",
    "'mask_args': {\"segment\": \"first\", \"add_one\": False},\n",
    "'segm_enc_args': {},\n",
    "'segm_predictor_args': {},\n",
    "'loss_args': {\"n_negatives\": 1, \"loss_args\": {\"reduction\": \"mean\"}},\n",
    "'num_epoch': 2,\n",
    "'use_projection': False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "88cce6e3",
   "metadata": {
    "cellId": "yco41l0mpilxwl1zo3lvn"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "class Conf:\n",
    "    def __init__(self, my_dict):\n",
    "        for key, value in my_dict.items():\n",
    "            setattr(self, key, value)\n",
    "            \n",
    "config = Conf(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c0b4f3b5",
   "metadata": {
    "cellId": "5r1y3v4wg5te6thuar14e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 240/240 [00:34<00:00,  6.97it/s]\n",
      "100%|██████████| 1237/1237 [03:08<00:00,  6.57it/s]\n",
      "100%|██████████| 1476/1476 [03:42<00:00,  6.62it/s]\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "model = FinModel1(config)\n",
    "checkpoint = torch.load(model_path)\n",
    "#     checkpoint = torch.load(model_path, map_location='cpu')\n",
    "\n",
    "state_dicts = OrderedDict()\n",
    "for key, value in checkpoint['state_dict'].items():\n",
    "    state_dicts[key.replace('wav2vec_segm.', '')] = value\n",
    "model.load_state_dict(state_dicts)\n",
    "\n",
    "model.eval()\n",
    "model=model.to('cuda')\n",
    "name_path = 'peak_detection'\n",
    "\n",
    "names = ['farfield', 'crowd', 'all']\n",
    "loaders = [test_farfield_loader, test_crowd_loader, test_loader]\n",
    "fin_path = os.path.join(path_results, name_path)\n",
    "\n",
    "\n",
    "for name, loader in zip(names, loaders):\n",
    "    for batch in tqdm(loader):\n",
    "        x = batch['batch']\n",
    "        lengths = batch['lengths']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        secs = batch['boundaries']\n",
    "        ids = batch['ids']\n",
    "        spectral_sizes = batch['spectral_size']\n",
    "        os.makedirs(os.path.join(fin_path, name), exist_ok=True) \n",
    "\n",
    "\n",
    "        rr = model.compute_all(x.to('cuda'), secs, num_epoch=0, \n",
    "                               spectral_size = spectral_sizes,\n",
    "                               attention_mask=attention_mask.to('cuda'), return_secs=True)\n",
    "    #         rr = model.compute_all(x, secs, num_epoch=0, \n",
    "#                                spectral_size = spectral_sizes,\n",
    "#         attention_mask=attention_mask, return_secs=True)\n",
    "\n",
    "        secs_preds = rr[1]['secs_pred']\n",
    "        for idd, secs in zip(ids, secs_preds):\n",
    "            with open(os.path.join(os.path.join(path_results, name_path), idd), 'w', encoding=\"cp1251\") as file:\n",
    "                file.write(str(list(secs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b57f3f0f",
   "metadata": {
    "cellId": "1h1teeyn0edifrphuxl6txa"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46e8d88",
   "metadata": {
    "cellId": "zlk1jjncevdv982fudjkrr"
   },
   "source": [
    "# Wav2Vec2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7b5b1f1b",
   "metadata": {
    "cellId": "emdh5ixho7pmlrxj1by1zi"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from modeling_segmentation import Wav2Vec2ModelForSegmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0bae9d03",
   "metadata": {
    "cellId": "eglzymolfnhrhg40r8wce"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "model_path = 'golos_model_segment_r_val_transformers_acc_10_ep_500_r_val_edges_train_10_hours_new_loss.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fa718113",
   "metadata": {
    "cellId": "lyyk8wja2wdfojzivnjzr"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing Wav2Vec2ModelForSegmentation: ['lm_head.bias', 'lm_head.weight']\n",
      "- This IS expected if you are initializing Wav2Vec2ModelForSegmentation from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ModelForSegmentation from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ModelForSegmentation were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 240/240 [01:16<00:00,  3.15it/s]\n",
      "100%|██████████| 1237/1237 [06:44<00:00,  3.06it/s]\n",
      "100%|██████████| 1476/1476 [07:59<00:00,  3.08it/s]\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "\n",
    "model = Wav2Vec2ModelForSegmentation.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "checkpoint = torch.load(model_path)\n",
    "#     checkpoint = torch.load(model_path, map_location='cpu')\n",
    "\n",
    "state_dicts = OrderedDict()\n",
    "for key, value in checkpoint['state_dict'].items():\n",
    "    if 'project_back' not in key:\n",
    "        state_dicts[key.replace('wav2vec_segm.', '')] = value\n",
    "model.load_state_dict(state_dicts)\n",
    "\n",
    "model.eval()\n",
    "model=model.to('cuda')\n",
    "name_path = 'wav2vec_model'\n",
    "\n",
    "names = ['farfield', 'crowd', 'all']\n",
    "loaders = [test_farfield_loader, test_crowd_loader, test_loader]\n",
    "fin_path = os.path.join(path_results, name_path)\n",
    "\n",
    "\n",
    "for name, loader in zip(names, loaders):\n",
    "    for batch in tqdm(loader):\n",
    "        x = batch['batch']\n",
    "        lengths = batch['lengths']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        secs = batch['boundaries']\n",
    "        ids = batch['ids']\n",
    "        spectral_sizes = batch['spectral_size']\n",
    "        os.makedirs(os.path.join(fin_path, name), exist_ok=True) \n",
    "\n",
    "        rr = model.compute_all(x.to('cuda'), secs, num_epoch=0, attention_mask=attention_mask.to('cuda'), return_secs=True)\n",
    "    #         rr = model.compute_all(x, secs, num_epoch=0, attention_mask=attention_mask, return_secs=True)\n",
    "        secs_preds = rr[1]['secs_pred']\n",
    "        for idd, secs in zip(ids, secs_preds):\n",
    "            with open(os.path.join(os.path.join(path_results, name_path), idd), 'w', encoding=\"cp1251\") as file:\n",
    "                file.write(str(secs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3535016",
   "metadata": {
    "cellId": "ekhltku03k9kirs69z76lg"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3a999c",
   "metadata": {
    "cellId": "f4xi18mu4cz8msfyeinr8"
   },
   "source": [
    "# Агрегация данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dd67ca70",
   "metadata": {
    "cellId": "mebr93yvceqkw5udf7obi"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2a021ce0",
   "metadata": {
    "cellId": "th5elqnthpo9pgej9czjqe"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "# Данная функция основана на https://github.com/felixkreuk/UnsupSeg/blob/master/utils.py\n",
    "\n",
    "class RMetrics1(nn.Module):\n",
    "    def __init__(self, eps = 1e-5, tolerance = 2, sampling_rate = 16000):\n",
    "        super(RMetrics1, self).__init__()\n",
    "        self.tolerance = tolerance\n",
    "        self.eps = eps\n",
    "        self.sampling_rate = sampling_rate\n",
    "    \n",
    "    def calculate_stride(self, isz, conv_layers):\n",
    "        pad = 0\n",
    "        insize = isz\n",
    "        totstride = 1\n",
    "        sec_per_frame = 1/self.sampling_rate\n",
    "\n",
    "        for layer in conv_layers:\n",
    "            kernel, stride = layer\n",
    "            outsize = (insize + 2*pad - 1*(kernel-1)-1) / stride + 1\n",
    "            insize = outsize\n",
    "            totstride = totstride * stride\n",
    "\n",
    "        RFsize = isz - (outsize - 1) * totstride\n",
    "\n",
    "        ms_per_frame = sec_per_frame*RFsize*1000\n",
    "        ms_stride = sec_per_frame*totstride*1000\n",
    "        return outsize, totstride, RFsize, ms_per_frame, ms_stride\n",
    "        \n",
    "    def get_frames(self, secs, stride):\n",
    "        frames = [[int(i*self.sampling_rate/stride) for i in sec] for sec in secs]\n",
    "        return frames\n",
    "        \n",
    "    def make_true_boundaries(self, secs, boundaries, stride):\n",
    "        frames = self.get_frames(secs, stride)\n",
    "        true_boundaries = torch.zeros(size = boundaries.shape)\n",
    "        for num_frame, frame in enumerate(frames):\n",
    "            for i in frame:\n",
    "                true_boundaries[num_frame, i] = 1\n",
    "        return true_boundaries.long().detach().numpy()\n",
    "    \n",
    "    def get_sec_bounds(self, b, stride, attention_mask = None):\n",
    "        if type(b)==torch.Tensor:\n",
    "            b1 = b.long().detach().cpu().numpy()\n",
    "        else:\n",
    "            b1 = b\n",
    "        \n",
    "        if attention_mask is not None:\n",
    "            b1 = b1*attention_mask.long().detach().cpu().numpy()\n",
    "            \n",
    "        frames_pred = []\n",
    "        secs_pred = []\n",
    "        for i in range(b1.shape[0]):\n",
    "            frames = np.where(b1[i, :] == 1)[0]\n",
    "            secs = [i*stride/self.sampling_rate for i in frames]\n",
    "            frames_pred.append(frames)\n",
    "            secs_pred.append(secs)\n",
    "        return frames_pred, secs_pred\n",
    "    \n",
    "    def get_precision_recall_frames(self, true_boundaries, b, attention_mask = None):\n",
    "        if type(b)==torch.Tensor:\n",
    "            b1 = b.long().detach().numpy()\n",
    "        else:\n",
    "            b1 = b\n",
    "            \n",
    "        if attention_mask is not None:\n",
    "            b1 = b1*attention_mask.long().detach().cpu().numpy()\n",
    "            \n",
    "        recall = recall_score(true_boundaries.flatten(), b1.flatten())\n",
    "        pre = precision_score(true_boundaries.flatten(), b1.flatten())\n",
    "        f_score = f1_score(true_boundaries.flatten(), b1.flatten())\n",
    "        return recall, pre, f_score\n",
    "    \n",
    "    def get_stats(self, frames_true, frames_pred):\n",
    "        \n",
    "        # Утащено отсюда: https://github.com/felixkreuk/UnsupSeg/blob/68c2c7b9bd49f3fb8f51c5c2f4d5aa85f251eaa8/utils.py#L69\n",
    "        precision_counter = 0 \n",
    "        recall_counter = 0\n",
    "        pred_counter = 0 \n",
    "        gt_counter = 0\n",
    "\n",
    "        for (y, yhat) in zip(frames_true, frames_pred):\n",
    "            for yhat_i in yhat:\n",
    "                min_dist = np.abs(np.array(y) - yhat_i).min()\n",
    "                precision_counter += (min_dist <= self.tolerance)\n",
    "            for y_i in y:\n",
    "                min_dist = np.abs(np.array(yhat) - y_i).min()\n",
    "                recall_counter += (min_dist <= self.tolerance)\n",
    "            pred_counter += len(yhat)\n",
    "            gt_counter += len(y)\n",
    "\n",
    "        return precision_counter, recall_counter, pred_counter, gt_counter\n",
    "    \n",
    "    def calc_metr(self, precision_counter, recall_counter, pred_counter, gt_counter):\n",
    "\n",
    "        # Утащено отсюда: https://github.com/felixkreuk/UnsupSeg/blob/68c2c7b9bd49f3fb8f51c5c2f4d5aa85f251eaa8/utils.py#L69\n",
    "        EPS = 1e-7\n",
    "\n",
    "        precision = precision_counter / (pred_counter + self.eps)\n",
    "        recall = recall_counter / (gt_counter + self.eps)\n",
    "        f1 = 2 * (precision * recall) / (precision + recall + self.eps)\n",
    "\n",
    "        os = recall / (precision + EPS) - 1\n",
    "        r1 = np.sqrt((1 - recall) ** 2 + os ** 2)\n",
    "        r2 = (-os + recall - 1) / (np.sqrt(2))\n",
    "        rval = 1 - (np.abs(r1) + np.abs(r2)) / 2\n",
    "\n",
    "        return precision, recall, f1, rval\n",
    "    \n",
    "    def get_metrics(self, true_secs, b, seq_len, config, attention_mask = None, \n",
    "                    return_secs=False):\n",
    "        \n",
    "        outsize, totstride, RFsize, ms_per_frame, ms_stride = self.calculate_stride(seq_len, config)\n",
    "#         print(seq_len, outsize, totstride, RFsize, ms_per_frame, ms_stride)\n",
    "        frames_true = self.get_frames(true_secs, totstride)\n",
    "        frames_pred, secs_pred = self.get_sec_bounds(b, totstride, attention_mask = attention_mask)\n",
    "        precision_counter, recall_counter, pred_counter, gt_counter = self.get_stats(frames_true, frames_pred)\n",
    "        precision, recall, f1, rval = self.calc_metr(precision_counter, recall_counter, pred_counter, gt_counter)\n",
    "        if return_secs:\n",
    "            return precision, recall, f1, rval, secs_pred\n",
    "        else:\n",
    "            return precision, recall, f1, rval\n",
    "        \n",
    "    def get_metrics_secs(self, true_secs, secs_pred, totstride = 160):\n",
    "        \n",
    "        frames_true = self.get_frames(true_secs, totstride)\n",
    "        frames_pred = self.get_frames(secs_pred, totstride)\n",
    "        precision_counter, recall_counter, pred_counter, gt_counter = self.get_stats(frames_true, frames_pred)\n",
    "        precision, recall, f1, rval = self.calc_metr(precision_counter, recall_counter, pred_counter, gt_counter)\n",
    "        return precision, recall, f1, rval\n",
    "    \n",
    "    def get_metrics_secs1(self, true_secs, secs_pred, totstride = 160):\n",
    "        \n",
    "#         frames_true = self.get_frames(true_secs, totstride)\n",
    "#         frames_pred = self.get_frames(secs_pred, totstride)\n",
    "        precision_counter, recall_counter, pred_counter, gt_counter = self.get_stats(true_secs, secs_pred)\n",
    "        precision, recall, f1, rval = self.calc_metr(precision_counter, recall_counter, pred_counter, gt_counter)\n",
    "        return precision, recall, f1, rval\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5652165c",
   "metadata": {
    "cellId": "wtfkbwahnksofpkju9hze"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "def read_true_file(segment_file):\n",
    "    with open(segment_file, 'r', encoding=\"cp1251\") as file:\n",
    "        tt = file.read()\n",
    "\n",
    "    boundaries = set()\n",
    "    mm = [i for i in tt.split('\\n') if len(i)>0]\n",
    "    for i in mm:\n",
    "        boundaries.add(eval(i.split()[0]))\n",
    "        boundaries.add(eval(i.split()[1]))\n",
    "    boundaries = sorted(list(boundaries))\n",
    "    return boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "64c3c5d4",
   "metadata": {
    "cellId": "xryrry7iibatmhwpmq6e0c"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "def read_pred_file(segment_file):\n",
    "    with open(segment_file, 'r', encoding=\"cp1251\") as file:\n",
    "        tt = file.read()\n",
    "    boundaries = eval(tt)\n",
    "    return boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a00b82c2",
   "metadata": {
    "cellId": "luczu6jxy7jqfo0u1md5os"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "def collate_fn_res(samples):\n",
    "\n",
    "    boundaries = [sample['boundaries'] for sample in samples]\n",
    "    samples1 = []\n",
    "    lengths = []\n",
    "    samplings = []\n",
    "    attentions = []\n",
    "    ids = []\n",
    "    audio_files = []\n",
    "    segment_files = []\n",
    "    for sample in samples:\n",
    "        ids.append(sample['id'])\n",
    "        audio_files.append(sample['audio_file'])\n",
    "        segment_files.append(sample['segment_file'])\n",
    "        \n",
    "    \n",
    "    return dict(batch=batch, lengths=lengths, \n",
    "                boundaries=boundaries, ids=ids, \n",
    "                audio_files=audio_files, \n",
    "                segment_files=segment_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e86022c6",
   "metadata": {
    "cellId": "xvouexqk3isi2o6erv59ra"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "test_farfield_dataset = Dataset_test('test/', 'segments_edges_init_test/', 'manifest_test_farfield.json', \n",
    "                        edges_path = 'silero_edges_test',\n",
    "                              chars = 'segments_chars/', frames = 'secs/')\n",
    "test_crowd_dataset = Dataset_test('test/', 'segments_edges_init_test/', 'manifest_test_crowd.json', \n",
    "                        edges_path = 'silero_edges_test',\n",
    "                              chars = 'segments_chars/', frames = 'secs/')\n",
    "test_dataset = Dataset_test('test/', 'segments_edges_init_test/', 'manifest_test.json', \n",
    "                        edges_path = 'silero_edges_test',\n",
    "                              chars = 'segments_chars/', frames = 'secs/')\n",
    "\n",
    "test_farfield_loader = DataLoader(test_farfield_dataset, shuffle=True, batch_size=8, collate_fn = collate_fn)\n",
    "test_crowd_loader = DataLoader(test_crowd_dataset, shuffle=True, batch_size=8, collate_fn = collate_fn)\n",
    "test_loader = DataLoader(test_dataset, shuffle=True, batch_size=8, collate_fn = collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "85b685f1",
   "metadata": {
    "cellId": "5z1h5gnt6pyy7q6ahspfdn"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "321e9960",
   "metadata": {
    "cellId": "0a1bbp1cnctqz6d70x9ex8"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "path_results = 'save_results_path_compare_golos'\n",
    "\n",
    "folders = ['model_segment', 'peak_detection', 'wav2vec_model']\n",
    "names = ['farfield', 'crowd', 'all']\n",
    "loaders = [test_farfield_loader, test_crowd_loader, test_loader]\n",
    "datasets = [test_farfield_dataset, test_crowd_dataset, test_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "465dc25e",
   "metadata": {
    "cellId": "7ycog6owhl50b694md39ui8"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0aba353b",
   "metadata": {
    "cellId": "n5odnxatoeou1o09k14kr"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1915/1915 [00:07<00:00, 271.00it/s]\n",
      "100%|██████████| 9889/9889 [00:58<00:00, 168.33it/s]\n",
      "100%|██████████| 11804/11804 [01:01<00:00, 191.20it/s]\n",
      "100%|██████████| 1915/1915 [00:07<00:00, 251.88it/s]\n",
      "100%|██████████| 9889/9889 [00:49<00:00, 199.04it/s]\n",
      "100%|██████████| 11804/11804 [00:53<00:00, 222.22it/s]\n",
      "100%|██████████| 1915/1915 [00:07<00:00, 273.14it/s]\n",
      "100%|██████████| 9889/9889 [00:47<00:00, 206.91it/s]\n",
      "100%|██████████| 11804/11804 [00:55<00:00, 214.41it/s]\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "result_dataframes = []\n",
    "metr = RMetrics1(tolerance=2)\n",
    "totstride = 160\n",
    "\n",
    "for folder in folders:\n",
    "    path_folder = os.path.join(path_results, folder)\n",
    "    for name, dataset in zip(names, datasets):\n",
    "#         name_path = os.path.join(path_folder, name)\n",
    "        idss = os.listdir(path_folder)\n",
    "\n",
    "        secs_preds = []\n",
    "        secs_trues = []\n",
    "\n",
    "        for num in tqdm(range(len(dataset))):\n",
    "\n",
    "            idd = dataset[num]\n",
    "            true_file = idd['segment_file']\n",
    "            bound_true = idd['boundaries']\n",
    "            ids = idd['id']\n",
    "\n",
    "    #         bound_true = read_true_file(true_files[num])\n",
    "            bound_pred = read_pred_file(os.path.join(path_folder, ids))\n",
    "\n",
    "            secs_trues.append(bound_true)\n",
    "            secs_preds.append(bound_pred)\n",
    "\n",
    "        precision, recall, f1, rval = metr.get_metrics_secs(secs_trues, secs_preds, totstride = totstride)\n",
    "\n",
    "        datafr = pd.DataFrame([folder, name, \n",
    "                               precision, recall, \n",
    "                               f1, rval]).T.rename(columns = {0:'type', \n",
    "                                                              1:'name',\n",
    "                                                              2:'precision',\n",
    "                                                              3:'recall', \n",
    "                                                              4:'f1', \n",
    "                                                              5:'rval'})\n",
    "\n",
    "        result_dataframes.append(datafr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b0b368",
   "metadata": {
    "cellId": "fqfg6q5qs4fvn2z6vhq7"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ba7244e9",
   "metadata": {
    "cellId": "ofix0y0jgol8v2nbs8p4"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "result_df = pd.concat(result_dataframes, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a247b989",
   "metadata": {
    "cellId": "pk2m5upkyk5ma3srsbl1q"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>rval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>model_segment</td>\n",
       "      <td>farfield</td>\n",
       "      <td>0.569363</td>\n",
       "      <td>0.581862</td>\n",
       "      <td>0.57554</td>\n",
       "      <td>0.635048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>model_segment</td>\n",
       "      <td>crowd</td>\n",
       "      <td>0.562237</td>\n",
       "      <td>0.581969</td>\n",
       "      <td>0.571928</td>\n",
       "      <td>0.630045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>model_segment</td>\n",
       "      <td>all</td>\n",
       "      <td>0.563326</td>\n",
       "      <td>0.581952</td>\n",
       "      <td>0.572482</td>\n",
       "      <td>0.630831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>peak_detection</td>\n",
       "      <td>farfield</td>\n",
       "      <td>0.562177</td>\n",
       "      <td>0.818671</td>\n",
       "      <td>0.666597</td>\n",
       "      <td>0.5291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>peak_detection</td>\n",
       "      <td>crowd</td>\n",
       "      <td>0.555358</td>\n",
       "      <td>0.565633</td>\n",
       "      <td>0.560443</td>\n",
       "      <td>0.622506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>peak_detection</td>\n",
       "      <td>all</td>\n",
       "      <td>0.556949</td>\n",
       "      <td>0.605537</td>\n",
       "      <td>0.580223</td>\n",
       "      <td>0.627695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>wav2vec_model</td>\n",
       "      <td>farfield</td>\n",
       "      <td>0.661687</td>\n",
       "      <td>0.386808</td>\n",
       "      <td>0.488211</td>\n",
       "      <td>0.559747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>wav2vec_model</td>\n",
       "      <td>crowd</td>\n",
       "      <td>0.574688</td>\n",
       "      <td>0.48842</td>\n",
       "      <td>0.528049</td>\n",
       "      <td>0.605628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>wav2vec_model</td>\n",
       "      <td>all</td>\n",
       "      <td>0.584543</td>\n",
       "      <td>0.472396</td>\n",
       "      <td>0.522515</td>\n",
       "      <td>0.600593</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             type      name precision    recall        f1      rval\n",
       "0   model_segment  farfield  0.569363  0.581862   0.57554  0.635048\n",
       "1   model_segment     crowd  0.562237  0.581969  0.571928  0.630045\n",
       "2   model_segment       all  0.563326  0.581952  0.572482  0.630831\n",
       "3  peak_detection  farfield  0.562177  0.818671  0.666597    0.5291\n",
       "4  peak_detection     crowd  0.555358  0.565633  0.560443  0.622506\n",
       "5  peak_detection       all  0.556949  0.605537  0.580223  0.627695\n",
       "6   wav2vec_model  farfield  0.661687  0.386808  0.488211  0.559747\n",
       "7   wav2vec_model     crowd  0.574688   0.48842  0.528049  0.605628\n",
       "8   wav2vec_model       all  0.584543  0.472396  0.522515  0.600593"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2c5f2932",
   "metadata": {
    "cellId": "7ruor5kru5lml3zpsuk5f"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "result_df.to_csv('results_compare_golos.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046c6841",
   "metadata": {
    "cellId": "c8a4aoohus5299svgtxam"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Yandex DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "notebookId": "8deb7d57-63f0-4c8c-910f-6736a98d91d5",
  "notebookPath": "Predict_compare_golos.ipynb",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
