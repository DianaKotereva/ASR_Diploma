{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9889756",
   "metadata": {
    "cellId": "hx4ilavsekb7p5j4apa0ad"
   },
   "source": [
    "# Импорты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "792b8e29",
   "metadata": {
    "cellId": "cnf87s891veud9gt2lxxl"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import scipy.io.wavfile as wav\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d34b4c5",
   "metadata": {
    "cellId": "oc77e6o7t1qsutuvez776"
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f3ac21a",
   "metadata": {
    "cellId": "75nk4sdqcfvl8a494j8ev"
   },
   "outputs": [],
   "source": [
    "from models import ConvFeatureEncoder, SegmentsRepr, SegmentsEncoder, NegativeSampler, SegmentPredictor, FinModel\n",
    "from utils import ConstrativeLoss, sample_negatives\n",
    "from collections import OrderedDict\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb2af9d4",
   "metadata": {
    "cellId": "dlkze2hpusrdlri7x2hivr"
   },
   "outputs": [],
   "source": [
    "from model_transformers import SegmentTransformer\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aab3bdb1",
   "metadata": {
    "cellId": "lkg6h7gwm2bgzpcn9nzv8s"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.14.0'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c0e7810",
   "metadata": {
    "cellId": "qmml82lztvsz43lm9lnsja"
   },
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    \n",
    "    def __init__(self, path, segment_path, manifest_path, edges_path = None, chars = 'segments_chars/', frames = 'secs/'):\n",
    "        with open(manifest_path, 'r') as json_file:\n",
    "            manifest = json.load(json_file)\n",
    "        self.manifest = manifest\n",
    "#         self.manifest = manifest[:1000]\n",
    "        self.path = path\n",
    "        self.segment_path = segment_path\n",
    "        self.frames = os.path.join(chars, frames)\n",
    "        self.edges_path = edges_path\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.manifest)\n",
    "    \n",
    "    def __getitem__(self, ind):\n",
    "#         print(self.manifest[ind])\n",
    "        \n",
    "        # Загрузка аудио-сигнала\n",
    "        \n",
    "        audio_filepath = self.manifest[ind]['audio_filepath']\n",
    "#         print(audio_filepath)\n",
    "        audio_file = os.path.join(self.path, audio_filepath)\n",
    "#         print(audio_file)\n",
    "        sampling_rate, signal = wav.read(audio_file)\n",
    "#         signal, sampling_rate = torchaudio.load(path)\n",
    "        \n",
    "        # Обрезать тишину\n",
    "        filetext_id = self.manifest[ind]['id']+'.txt'\n",
    "        silero_filepath = audio_filepath.replace('.wav', '.txt')\n",
    "        silero_file = os.path.join(self.edges_path, silero_filepath)\n",
    "#         print(silero_file)\n",
    "        \n",
    "        with open(silero_file, 'r', encoding=\"cp1251\") as file:\n",
    "            tt = file.read()\n",
    "        \n",
    "        start = []\n",
    "        end = []\n",
    "        for line in tt.split('\\n'):\n",
    "            if len(line) > 0:\n",
    "                start.append(eval(line.split()[0]))\n",
    "                end.append(eval(line.split()[1]))\n",
    "        start = min(start)\n",
    "        end = max(end)\n",
    "#         print(start)\n",
    "#         print(end)\n",
    "        signal = signal[start:end]\n",
    "        \n",
    "        # Загрузка разметки\n",
    "        filetext_id = self.manifest[ind]['id']+'.txt'\n",
    "        segment_filepath = audio_filepath.replace('.wav', '.txt').replace(filetext_id, '')\n",
    "        \n",
    "        segment_file = os.path.join(os.path.join(os.path.join(self.segment_path, segment_filepath), self.frames), filetext_id)\n",
    "#         print(segment_file)\n",
    "        with open(segment_file, 'r', encoding=\"cp1251\") as file:\n",
    "            tt = file.read()\n",
    "        \n",
    "        boundaries = set()\n",
    "        mm = [i for i in tt.split('\\n') if len(i)>0]\n",
    "        for i in mm:\n",
    "            boundaries.add(eval(i.split()[0]))\n",
    "            boundaries.add(eval(i.split()[1]))\n",
    "        boundaries = sorted(list(boundaries))\n",
    "        \n",
    "        return {'audio_file':os.path.join(self.path, audio_filepath), \n",
    "                'segment_file':segment_file, \n",
    "                'id':filetext_id, 'sample': signal, \n",
    "                'length': len(signal), 'boundaries': boundaries}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9cec4ee1",
   "metadata": {
    "cellId": "gfwg7tmhh5bzsaq582fpm"
   },
   "outputs": [],
   "source": [
    "class Dataset_test:\n",
    "    \n",
    "    def __init__(self, path, segment_path, manifest_path, edges_path = None, chars = 'segments_chars/', frames = 'secs/'):\n",
    "        with open(manifest_path, 'r') as json_file:\n",
    "            manifest = json.load(json_file)\n",
    "        self.manifest = manifest\n",
    "#         self.manifest = manifest[:1000]\n",
    "        self.path = path\n",
    "        self.segment_path = segment_path\n",
    "        self.frames = os.path.join(chars, frames)\n",
    "        self.edges_path = edges_path\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.manifest)\n",
    "    \n",
    "    def __getitem__(self, ind):\n",
    "#         print(self.manifest[ind])\n",
    "        \n",
    "        # Загрузка аудио-сигнала\n",
    "        \n",
    "        audio_filepath = self.manifest[ind]['audio_filepath']\n",
    "#         print(audio_filepath)\n",
    "        audio_file = os.path.join(self.path, audio_filepath)\n",
    "#         print(audio_file)\n",
    "        sampling_rate, signal = wav.read(audio_file)\n",
    "#         signal, sampling_rate = torchaudio.load(path)\n",
    "        \n",
    "        # Обрезать тишину\n",
    "        filetext_id = self.manifest[ind]['id']+'.txt'\n",
    "        silero_filepath = audio_filepath.replace('.wav', '.txt')\n",
    "        silero_file = os.path.join(self.edges_path, silero_filepath).replace('files/', '')\n",
    "#         print(silero_file)\n",
    "        \n",
    "        with open(silero_file, 'r', encoding=\"cp1251\") as file:\n",
    "            tt = file.read()\n",
    "        \n",
    "        start = []\n",
    "        end = []\n",
    "        for line in tt.split('\\n'):\n",
    "            if len(line) > 0:\n",
    "                start.append(eval(line.split()[0]))\n",
    "                end.append(eval(line.split()[1]))\n",
    "        start = min(start)\n",
    "        end = max(end)\n",
    "#         print(start)\n",
    "#         print(end)\n",
    "        signal = signal[start:end]\n",
    "        \n",
    "        # Загрузка разметки\n",
    "        filetext_id = self.manifest[ind]['id']+'.txt'\n",
    "        segment_filepath = audio_filepath.replace('.wav', '.txt').replace(filetext_id, '')\n",
    "        \n",
    "        segment_file = os.path.join(os.path.join(os.path.join(self.segment_path, segment_filepath), self.frames), filetext_id).replace('files/', '')\n",
    "#         print(segment_file)\n",
    "        with open(segment_file, 'r', encoding=\"cp1251\") as file:\n",
    "            tt = file.read()\n",
    "        \n",
    "        boundaries = set()\n",
    "        mm = [i for i in tt.split('\\n') if len(i)>0]\n",
    "        for i in mm:\n",
    "            boundaries.add(eval(i.split()[0]))\n",
    "            boundaries.add(eval(i.split()[1]))\n",
    "        boundaries = sorted(list(boundaries))\n",
    "        \n",
    "        return {'audio_file':os.path.join(self.path, audio_filepath), \n",
    "                'segment_file':segment_file, \n",
    "                'id':filetext_id, 'sample': signal, \n",
    "                'length': len(signal), 'boundaries': boundaries}\n",
    "        \n",
    "#         return {'id':filetext_id, 'sample': signal, 'length': len(signal), 'boundaries': boundaries}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "226c78ee",
   "metadata": {
    "cellId": "qvr0zh4yguera6re8fl1f"
   },
   "outputs": [],
   "source": [
    "def collate_fn(samples):\n",
    "    \n",
    "    max_length = max([sample['length'] for sample in samples])\n",
    "    boundaries = [sample['boundaries'] for sample in samples]\n",
    "    samples1 = []\n",
    "    lengths = []\n",
    "    samplings = []\n",
    "    attentions = []\n",
    "    ids = []\n",
    "    audio_files = []\n",
    "    segment_files = []\n",
    "    for sample in samples:\n",
    "        to_add_l = max_length-sample['length']\n",
    "        sample1 = list(sample['sample'])+[0]*to_add_l\n",
    "        samples1.append(torch.Tensor(sample1).unsqueeze(0))\n",
    "        lengths.append(sample['length'])\n",
    "        ids.append(sample['id'])\n",
    "        audio_files.append(sample['audio_file'])\n",
    "        segment_files.append(sample['segment_file'])\n",
    "        att_norm = torch.ones(size = (1, sample['length']))\n",
    "        att_add = torch.zeros(size = (1, to_add_l))\n",
    "        att = torch.cat([att_norm, att_add], dim = -1)\n",
    "        attentions.append(att)\n",
    "        \n",
    "    batch = torch.cat(samples1)\n",
    "    lengths = torch.Tensor(lengths)\n",
    "    attention_mask = torch.cat(attentions, dim = 0)\n",
    "    \n",
    "    return dict(batch=batch, lengths=lengths, attention_mask=attention_mask, \n",
    "                boundaries=boundaries, ids=ids, \n",
    "                audio_files=audio_files, \n",
    "                segment_files=segment_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6dda89",
   "metadata": {
    "cellId": "si84mcfqxf9vw88jq4dnx"
   },
   "source": [
    "# Predict Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1bbf0fde",
   "metadata": {
    "cellId": "6c5xz7m4h1rc8i4fugle0c"
   },
   "outputs": [],
   "source": [
    "train_dataset = Dataset('train/', 'segments_edges_init/', 'manifest_silero_edges_train1.json', \n",
    "                        edges_path = 'silero_edges',\n",
    "                              chars = 'segments_chars/', frames = 'secs/')\n",
    "val_dataset = Dataset('train/', 'segments_edges_init/', 'manifest_silero_edges_val1.json', \n",
    "                        edges_path = 'silero_edges',\n",
    "                              chars = 'segments_chars/', frames = 'secs/')\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=8, collate_fn = collate_fn)\n",
    "val_loader = DataLoader(val_dataset, shuffle=False, batch_size=8, collate_fn = collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6d21dc9",
   "metadata": {
    "cellId": "n5459u4c7o7dnc4dvw9w6q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "041e3b33",
   "metadata": {
    "cellId": "pckte08idi79u7de4gz33"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a1d0bbc",
   "metadata": {
    "cellId": "68f7eeo0o1ym3uvq8q9jxe"
   },
   "outputs": [],
   "source": [
    "path_to_save = 'save_results_path'\n",
    "path_audio = os.path.join(path_to_save, 'Audio')\n",
    "path_segments = os.path.join(path_to_save, 'Segment')\n",
    "path_results = os.path.join(path_to_save, 'Results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7cb5f6cf",
   "metadata": {
    "cellId": "k6bl8ogekrvces717m5tt"
   },
   "outputs": [],
   "source": [
    "os.makedirs(path_to_save, exist_ok=True) \n",
    "os.makedirs(path_audio, exist_ok=True) \n",
    "os.makedirs(path_segments, exist_ok=True) \n",
    "os.makedirs(path_results, exist_ok=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15d34af7",
   "metadata": {
    "cellId": "r8ivrkng5kmmni9ftvr9kp"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Audio', 'Segment', 'Results']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(path_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6092fded",
   "metadata": {
    "cellId": "geneu8n429sdabta8o7zug"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5dcbe84c",
   "metadata": {
    "cellId": "rr245v1euian3igs7zwol"
   },
   "outputs": [],
   "source": [
    "for batch in val_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "62035899",
   "metadata": {
    "cellId": "ilh62n15hglj2cfelks4k"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aaa8ca86",
   "metadata": {
    "cellId": "5f7wyndbo4nxgj1pw3tiod"
   },
   "outputs": [],
   "source": [
    "from transformers_f.src.transformers.activations import ACT2FN\n",
    "from transformers_f.src.transformers.models.wav2vec2.modeling_wav2vec2 import Wav2Vec2FeatureExtractor, Wav2Vec2Model\n",
    "\n",
    "from transformers_f.src.transformers.models.wav2vec2.modeling_segmentation import (Wav2Vec2ModelForSegmentation,\n",
    "                                                                                   SegmentsRepr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8ad4f5f1",
   "metadata": {
    "cellId": "sdpxmprodbi5f7rbttx8dd"
   },
   "outputs": [],
   "source": [
    "segment_paths = ['golos_model_segment_r_val_transformers_acc_10_ep_500_r_val_edges_train.ckpt']\n",
    "names_paths = ['edges']\n",
    "thres = [0.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "33576bc0",
   "metadata": {
    "cellId": "ba7l71rq11crxe5hojcjyo"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7033aa15",
   "metadata": {
    "cellId": "dc6jpndd8mfco3se4w9in5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80f46ae90e5246779c62339ea9fcc0ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=1596.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82ea544c548d49898540f184e5ac3bd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=377667514.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13623/13623 [1:26:31<00:00,  2.62it/s]\n"
     ]
    }
   ],
   "source": [
    "for model_path, name_path, thre in zip(segment_paths, names_paths, thres):\n",
    "    os.makedirs(os.path.join(path_results, name_path), exist_ok=True) \n",
    "    \n",
    "    wav2vec_segm = Wav2Vec2ModelForSegmentation.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "    wav2vec_segm.segment_mean = SegmentsRepr(thres = thre)\n",
    "    \n",
    "    checkpoint = torch.load(model_path)\n",
    "#     checkpoint = torch.load(model_path, map_location='cpu')\n",
    "    \n",
    "    state_dicts = OrderedDict()\n",
    "    for key, value in checkpoint['state_dict'].items():\n",
    "        state_dicts[key.replace('wav2vec_segm.', '')] = value\n",
    "    wav2vec_segm.load_state_dict(state_dicts)\n",
    "    \n",
    "    wav2vec_segm.eval()\n",
    "    wav2vec_segm=wav2vec_segm.to('cuda')\n",
    "    \n",
    "    for batch in tqdm(train_loader):\n",
    "        x = batch['batch']\n",
    "        lengths = batch['lengths']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        secs = batch['boundaries']\n",
    "        ids = batch['ids']\n",
    "        name_path = 'train'\n",
    "        os.makedirs(os.path.join(path_results, name_path), exist_ok=True) \n",
    "\n",
    "        rr = wav2vec_segm.compute_all(x.to('cuda'), secs, num_epoch=0, attention_mask=attention_mask.to('cuda'), return_secs=True)\n",
    "#         rr = wav2vec_segm.compute_all(x, secs, num_epoch=0, attention_mask=attention_mask, return_secs=True)\n",
    "        secs_preds = rr[1]['secs_pred']\n",
    "        for idd, secs in zip(ids, secs_preds):\n",
    "            with open(os.path.join(os.path.join(path_results, name_path), idd), 'w', encoding=\"cp1251\") as file:\n",
    "                file.write(str(secs))\n",
    "\n",
    "    for batch in tqdm(val_loader):\n",
    "        x = batch['batch']\n",
    "        lengths = batch['lengths']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        secs = batch['boundaries']\n",
    "        ids = batch['ids']\n",
    "        name_path = 'val'\n",
    "        os.makedirs(os.path.join(path_results, name_path), exist_ok=True) \n",
    "\n",
    "        rr = wav2vec_segm.compute_all(x.to('cuda'), secs, num_epoch=0, attention_mask=attention_mask.to('cuda'), return_secs=True)\n",
    "#         rr = wav2vec_segm.compute_all(x, secs, num_epoch=0, attention_mask=attention_mask, return_secs=True)\n",
    "        secs_preds = rr[1]['secs_pred']\n",
    "        for idd, secs in zip(ids, secs_preds):\n",
    "            with open(os.path.join(os.path.join(path_results, name_path), idd), 'w', encoding=\"cp1251\") as file:\n",
    "                file.write(str(secs))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "32264122",
   "metadata": {
    "cellId": "1uoazf7nvu72dagphnr19w"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa8493f1",
   "metadata": {
    "cellId": "p9fg5n0gzrock6y36ewspo"
   },
   "source": [
    "# Predict Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1969beb3",
   "metadata": {
    "cellId": "m260cbjx5xquckua910pc"
   },
   "outputs": [],
   "source": [
    "class Dataset_test:\n",
    "    \n",
    "    def __init__(self, path, segment_path, manifest_path, edges_path = None, chars = 'segments_chars/', frames = 'secs/'):\n",
    "        with open(manifest_path, 'r') as json_file:\n",
    "            manifest = json.load(json_file)\n",
    "        self.manifest = manifest\n",
    "#         self.manifest = manifest[:1000]\n",
    "        self.path = path\n",
    "        self.segment_path = segment_path\n",
    "        self.frames = os.path.join(chars, frames)\n",
    "        self.edges_path = edges_path\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.manifest)\n",
    "    \n",
    "    def __getitem__(self, ind):\n",
    "#         print(self.manifest[ind])\n",
    "        \n",
    "        # Загрузка аудио-сигнала\n",
    "        \n",
    "        audio_filepath = self.manifest[ind]['audio_filepath']\n",
    "#         print(audio_filepath)\n",
    "        audio_file = os.path.join(self.path, audio_filepath)\n",
    "#         print(audio_file)\n",
    "        sampling_rate, signal = wav.read(audio_file)\n",
    "#         signal, sampling_rate = torchaudio.load(path)\n",
    "        \n",
    "        # Обрезать тишину\n",
    "        filetext_id = self.manifest[ind]['id']+'.txt'\n",
    "        silero_filepath = audio_filepath.replace('.wav', '.txt')\n",
    "        silero_file = os.path.join(self.edges_path, silero_filepath).replace('files/', '')\n",
    "#         print(silero_file)\n",
    "        \n",
    "        with open(silero_file, 'r', encoding=\"cp1251\") as file:\n",
    "            tt = file.read()\n",
    "        \n",
    "        start = []\n",
    "        end = []\n",
    "        for line in tt.split('\\n'):\n",
    "            if len(line) > 0:\n",
    "                start.append(eval(line.split()[0]))\n",
    "                end.append(eval(line.split()[1]))\n",
    "        start = min(start)\n",
    "        end = max(end)\n",
    "#         print(start)\n",
    "#         print(end)\n",
    "        signal = signal[start:end]\n",
    "        \n",
    "        # Загрузка разметки\n",
    "        filetext_id = self.manifest[ind]['id']+'.txt'\n",
    "        segment_filepath = audio_filepath.replace('.wav', '.txt').replace(filetext_id, '')\n",
    "        \n",
    "        segment_file = os.path.join(os.path.join(os.path.join(self.segment_path, segment_filepath), self.frames), filetext_id).replace('files/', '')\n",
    "#         print(segment_file)\n",
    "        with open(segment_file, 'r', encoding=\"cp1251\") as file:\n",
    "            tt = file.read()\n",
    "        \n",
    "        boundaries = set()\n",
    "        mm = [i for i in tt.split('\\n') if len(i)>0]\n",
    "        for i in mm:\n",
    "            boundaries.add(eval(i.split()[0]))\n",
    "            boundaries.add(eval(i.split()[1]))\n",
    "        boundaries = sorted(list(boundaries))\n",
    "        \n",
    "        return {'audio_file':os.path.join(self.path, audio_filepath), \n",
    "                'segment_file':segment_file, \n",
    "                'id':filetext_id, 'sample': signal, \n",
    "                'length': len(signal), 'boundaries': boundaries}\n",
    "        \n",
    "#         return {'id':filetext_id, 'sample': signal, 'length': len(signal), 'boundaries': boundaries}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c07570db",
   "metadata": {
    "cellId": "rulah2g4vbh0qi5oiyz3g"
   },
   "outputs": [],
   "source": [
    "test_dataset = Dataset_test('test/', 'segments_edges_init_test/', 'manifest_test.json', \n",
    "                        edges_path = 'silero_edges_test',\n",
    "                              chars = 'segments_chars/', frames = 'secs/')\n",
    "\n",
    "test_loader = DataLoader(test_dataset, shuffle=True, batch_size=8, collate_fn = collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1731381",
   "metadata": {
    "cellId": "ae2a60e3tmd4rz6kw1ugel"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1abab1ae",
   "metadata": {
    "cellId": "if2n83cgrgeal40wg2w1e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d81164ab",
   "metadata": {
    "cellId": "oq4sjb4ooklcen7bsngbua"
   },
   "outputs": [],
   "source": [
    "from transformers_f.src.transformers.activations import ACT2FN\n",
    "from transformers_f.src.transformers.models.wav2vec2.modeling_wav2vec2 import Wav2Vec2FeatureExtractor, Wav2Vec2Model\n",
    "\n",
    "from transformers_f.src.transformers.models.wav2vec2.modeling_segmentation import (Wav2Vec2ModelForSegmentation,\n",
    "                                                                                   SegmentsRepr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b94f62c7",
   "metadata": {
    "cellId": "tr8h06zm0lfu9lqokq0ly"
   },
   "outputs": [],
   "source": [
    "for batch in test_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8b37cdf",
   "metadata": {
    "cellId": "eev918xpi9vtz9ay6n8zlf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6d1446f",
   "metadata": {
    "cellId": "r5zpolo0edonqewwpxjeo"
   },
   "outputs": [],
   "source": [
    "\n",
    "segment_paths = ['golos_model_segment_r_val_transformers_acc_10_ep_500_r_val_edges_train.ckpt']\n",
    "names_paths = ['edges']\n",
    "thres = [0.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "441c559d",
   "metadata": {
    "cellId": "u4kzeaxce7irp9v36bpwj"
   },
   "outputs": [],
   "source": [
    "path_to_save = 'save_results_path'\n",
    "path_results = os.path.join(path_to_save, 'Results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f05baf8e",
   "metadata": {
    "cellId": "avw5bdcb2dc8tujy9huzxn"
   },
   "outputs": [],
   "source": [
    "os.makedirs(path_to_save, exist_ok=True) \n",
    "os.makedirs(path_results, exist_ok=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19c779da",
   "metadata": {
    "cellId": "ekhpo4pu0z6jaledvwllip"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8201cbe",
   "metadata": {
    "cellId": "5ln90gb01na8c7rkovk944"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "50d0ebff",
   "metadata": {
    "cellId": "xn1rtcra2njs3x4kfuimb"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a957d5e2c4a147f88c25eb98ced29f36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=1596.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9b89f1bfd4847be924fc68506964c68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=377667514.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing Wav2Vec2ModelForSegmentation: ['lm_head.bias', 'lm_head.weight']\n",
      "- This IS expected if you are initializing Wav2Vec2ModelForSegmentation from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ModelForSegmentation from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ModelForSegmentation were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 1476/1476 [08:07<00:00,  3.03it/s]\n"
     ]
    }
   ],
   "source": [
    "for model_path, name_path, thre in zip(segment_paths, names_paths, thres):\n",
    "    os.makedirs(os.path.join(path_results, name_path), exist_ok=True) \n",
    "    \n",
    "    wav2vec_segm = Wav2Vec2ModelForSegmentation.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "    wav2vec_segm.segment_mean = SegmentsRepr(thres = thre)\n",
    "    \n",
    "    checkpoint = torch.load(model_path)\n",
    "#     checkpoint = torch.load(model_path, map_location='cpu')\n",
    "    \n",
    "    state_dicts = OrderedDict()\n",
    "    for key, value in checkpoint['state_dict'].items():\n",
    "        state_dicts[key.replace('wav2vec_segm.', '')] = value\n",
    "    wav2vec_segm.load_state_dict(state_dicts)\n",
    "    \n",
    "    wav2vec_segm.eval()\n",
    "    wav2vec_segm=wav2vec_segm.to('cuda')\n",
    "    \n",
    "    for batch in tqdm(test_loader):\n",
    "        x = batch['batch']\n",
    "        lengths = batch['lengths']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        secs = batch['boundaries']\n",
    "        ids = batch['ids']\n",
    "        name_path = 'test2'\n",
    "        os.makedirs(os.path.join(path_results, name_path), exist_ok=True) \n",
    "\n",
    "        rr = wav2vec_segm.compute_all(x.to('cuda'), secs, num_epoch=0, attention_mask=attention_mask.to('cuda'), return_secs=True)\n",
    "#         rr = wav2vec_segm.compute_all(x, secs, num_epoch=0, attention_mask=attention_mask, return_secs=True)\n",
    "        secs_preds = rr[1]['secs_pred']\n",
    "        for idd, secs in zip(ids, secs_preds):\n",
    "            with open(os.path.join(os.path.join(path_results, name_path), idd), 'w', encoding=\"cp1251\") as file:\n",
    "                file.write(str(secs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1305ca32",
   "metadata": {
    "cellId": "nbtj4lbdfxrcxbhlr4or7"
   },
   "source": [
    "# Get results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2e0a5cec",
   "metadata": {
    "cellId": "bokcdvz728ev45v0bj4kjd"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ba47df4f",
   "metadata": {
    "cellId": "pip9xt7km4f9rg1ycbhyat"
   },
   "outputs": [],
   "source": [
    "class RMetrics1(nn.Module):\n",
    "    def __init__(self, eps = 1e-5, tolerance = 2, sampling_rate = 16000):\n",
    "        super(RMetrics1, self).__init__()\n",
    "        self.tolerance = tolerance\n",
    "        self.eps = eps\n",
    "        self.sampling_rate = sampling_rate\n",
    "    \n",
    "    def calculate_stride(self, isz, config):\n",
    "        pad = 0\n",
    "        insize = isz\n",
    "        totstride = 1\n",
    "        sec_per_frame = 1/self.sampling_rate\n",
    "\n",
    "        for kernel, stride in zip(config.conv_kernel, config.conv_stride):\n",
    "            outsize = (insize + 2*pad - 1*(kernel-1)-1) / stride + 1\n",
    "            insize = outsize\n",
    "            totstride = totstride * stride\n",
    "\n",
    "        RFsize = isz - (outsize - 1) * totstride\n",
    "\n",
    "        ms_per_frame = sec_per_frame*RFsize*1000\n",
    "        ms_stride = sec_per_frame*totstride*1000\n",
    "        return outsize, totstride, RFsize, ms_per_frame, ms_stride\n",
    "        \n",
    "    def get_frames(self, secs, stride):\n",
    "        frames = [[int(i*self.sampling_rate/stride) for i in sec] for sec in secs]\n",
    "        return frames\n",
    "        \n",
    "    def make_true_boundaries(self, secs, boundaries, stride):\n",
    "        frames = self.get_frames(secs, stride)\n",
    "        true_boundaries = torch.zeros(size = boundaries.shape)\n",
    "        for num_frame, frame in enumerate(frames):\n",
    "            for i in frame:\n",
    "                true_boundaries[num_frame, i] = 1\n",
    "        return true_boundaries.long().detach().numpy()\n",
    "    \n",
    "    def get_sec_bounds(self, b, stride, attention_mask = None):\n",
    "        if type(b)==torch.Tensor:\n",
    "            b1 = b.long().detach().cpu().numpy()\n",
    "        else:\n",
    "            b1 = b\n",
    "        \n",
    "        if attention_mask is not None:\n",
    "            b1 = b1*attention_mask.long().detach().cpu().numpy()\n",
    "            \n",
    "        frames_pred = []\n",
    "        secs_pred = []\n",
    "        for i in range(b1.shape[0]):\n",
    "            frames = np.where(b1[i, :] == 1)[0]\n",
    "            secs = [i*stride/self.sampling_rate for i in frames]\n",
    "            frames_pred.append(frames)\n",
    "            secs_pred.append(secs)\n",
    "        return frames_pred, secs_pred\n",
    "    \n",
    "    def get_precision_recall_frames(self, true_boundaries, b, attention_mask = None):\n",
    "        if type(b)==torch.Tensor:\n",
    "            b1 = b.long().detach().numpy()\n",
    "        else:\n",
    "            b1 = b\n",
    "            \n",
    "        if attention_mask is not None:\n",
    "            b1 = b1*attention_mask.long().detach().cpu().numpy()\n",
    "            \n",
    "        recall = recall_score(true_boundaries.flatten(), b1.flatten())\n",
    "        pre = precision_score(true_boundaries.flatten(), b1.flatten())\n",
    "        f_score = f1_score(true_boundaries.flatten(), b1.flatten())\n",
    "        return recall, pre, f_score\n",
    "    \n",
    "    def get_stats(self, frames_true, frames_pred):\n",
    "        \n",
    "        # Утащено отсюда: https://github.com/felixkreuk/UnsupSeg/blob/68c2c7b9bd49f3fb8f51c5c2f4d5aa85f251eaa8/utils.py#L69\n",
    "        precision_counter = 0 \n",
    "        recall_counter = 0\n",
    "        pred_counter = 0 \n",
    "        gt_counter = 0\n",
    "\n",
    "        for (y, yhat) in zip(frames_true, frames_pred):\n",
    "            for yhat_i in yhat:\n",
    "                min_dist = np.abs(np.array(y) - yhat_i).min()\n",
    "                precision_counter += (min_dist <= self.tolerance)\n",
    "            for y_i in y:\n",
    "                min_dist = np.abs(np.array(yhat) - y_i).min()\n",
    "                recall_counter += (min_dist <= self.tolerance)\n",
    "            pred_counter += len(yhat)\n",
    "            gt_counter += len(y)\n",
    "\n",
    "        return precision_counter, recall_counter, pred_counter, gt_counter\n",
    "    \n",
    "    def calc_metr(self, precision_counter, recall_counter, pred_counter, gt_counter):\n",
    "\n",
    "        # Утащено отсюда: https://github.com/felixkreuk/UnsupSeg/blob/68c2c7b9bd49f3fb8f51c5c2f4d5aa85f251eaa8/utils.py#L69\n",
    "        EPS = 1e-7\n",
    "\n",
    "        precision = precision_counter / (pred_counter + self.eps)\n",
    "        recall = recall_counter / (gt_counter + self.eps)\n",
    "        f1 = 2 * (precision * recall) / (precision + recall + self.eps)\n",
    "\n",
    "        os = recall / (precision + EPS) - 1\n",
    "        r1 = np.sqrt((1 - recall) ** 2 + os ** 2)\n",
    "        r2 = (-os + recall - 1) / (np.sqrt(2))\n",
    "        rval = 1 - (np.abs(r1) + np.abs(r2)) / 2\n",
    "\n",
    "        return precision, recall, f1, rval\n",
    "    \n",
    "    def get_metrics(self, true_secs, b, seq_len, config, attention_mask = None, \n",
    "                    return_secs=False):\n",
    "        \n",
    "        outsize, totstride, RFsize, ms_per_frame, ms_stride = self.calculate_stride(seq_len, config)\n",
    "#         print(seq_len, outsize, totstride, RFsize, ms_per_frame, ms_stride)\n",
    "        frames_true = self.get_frames(true_secs, totstride)\n",
    "        frames_pred, secs_pred = self.get_sec_bounds(b, totstride, attention_mask = attention_mask)\n",
    "        precision_counter, recall_counter, pred_counter, gt_counter = self.get_stats(frames_true, frames_pred)\n",
    "        precision, recall, f1, rval = self.calc_metr(precision_counter, recall_counter, pred_counter, gt_counter)\n",
    "        if return_secs:\n",
    "            return precision, recall, f1, rval, secs_pred\n",
    "        else:\n",
    "            return precision, recall, f1, rval\n",
    "        \n",
    "    def get_metrics_secs(self, true_secs, secs_pred, totstride = 160):\n",
    "        \n",
    "        frames_true = self.get_frames(true_secs, totstride)\n",
    "        frames_pred = self.get_frames(secs_pred, totstride)\n",
    "        precision_counter, recall_counter, pred_counter, gt_counter = self.get_stats(frames_true, frames_pred)\n",
    "        precision, recall, f1, rval = self.calc_metr(precision_counter, recall_counter, pred_counter, gt_counter)\n",
    "        return precision, recall, f1, rval\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "18dc27fd",
   "metadata": {
    "cellId": "vhtwznk2mbjwspm649bpq"
   },
   "outputs": [],
   "source": [
    "def read_true_file(segment_file):\n",
    "    with open(segment_file, 'r', encoding=\"cp1251\") as file:\n",
    "        tt = file.read()\n",
    "\n",
    "    boundaries = set()\n",
    "    mm = [i for i in tt.split('\\n') if len(i)>0]\n",
    "    for i in mm:\n",
    "        boundaries.add(eval(i.split()[0]))\n",
    "        boundaries.add(eval(i.split()[1]))\n",
    "    boundaries = sorted(list(boundaries))\n",
    "    return boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "44a0310d",
   "metadata": {
    "cellId": "40kryrbbjnkedurh7viv5n"
   },
   "outputs": [],
   "source": [
    "def read_pred_file(segment_file):\n",
    "    with open(segment_file, 'r', encoding=\"cp1251\") as file:\n",
    "        tt = file.read()\n",
    "    boundaries = eval(tt)\n",
    "    if len(boundaries)>1:\n",
    "        return boundaries[1:]\n",
    "    else:\n",
    "        return boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4b6e83d2",
   "metadata": {
    "cellId": "snzs3mssc1hqjdilocb2"
   },
   "outputs": [],
   "source": [
    "class Dataset_res:\n",
    "    \n",
    "    def __init__(self, path, segment_path, manifest_path, chars = 'segments_chars/', frames = 'secs/'):\n",
    "        with open(manifest_path, 'r') as json_file:\n",
    "            manifest = json.load(json_file)\n",
    "        self.manifest = manifest\n",
    "        self.path = path\n",
    "        self.segment_path = segment_path\n",
    "        self.frames = os.path.join(chars, frames)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.manifest)\n",
    "    \n",
    "    def __getitem__(self, ind):\n",
    "#         print(self.manifest[ind])\n",
    "        \n",
    "        # Загрузка аудио-сигнала\n",
    "        \n",
    "        audio_filepath = self.manifest[ind]['audio_filepath']\n",
    "#         print(audio_filepath)\n",
    "        audio_file = os.path.join(self.path, audio_filepath)\n",
    "    \n",
    "        filetext_id = self.manifest[ind]['id']+'.txt'\n",
    "        segment_filepath = audio_filepath.replace('.wav', '.txt').replace(filetext_id, '')\n",
    "        \n",
    "        segment_file = os.path.join(os.path.join(os.path.join(self.segment_path, segment_filepath), self.frames), filetext_id).replace('files/', '')\n",
    "#         print(segment_file)\n",
    "        with open(segment_file, 'r', encoding=\"cp1251\") as file:\n",
    "            tt = file.read()\n",
    "        \n",
    "        boundaries = set()\n",
    "        mm = [i for i in tt.split('\\n') if len(i)>0]\n",
    "        for i in mm:\n",
    "            boundaries.add(eval(i.split()[0]))\n",
    "            boundaries.add(eval(i.split()[1]))\n",
    "        boundaries = sorted(list(boundaries))\n",
    "        \n",
    "        return {'segment_file':segment_file, \n",
    "                'id':filetext_id,\n",
    "                'boundaries': boundaries}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5d947ee9",
   "metadata": {
    "cellId": "leaa0dnmu7f0hlrmmwn2mgu"
   },
   "outputs": [],
   "source": [
    "def collate_fn_res(samples):\n",
    "\n",
    "    boundaries = [sample['boundaries'] for sample in samples]\n",
    "    samples1 = []\n",
    "    lengths = []\n",
    "    samplings = []\n",
    "    attentions = []\n",
    "    ids = []\n",
    "    audio_files = []\n",
    "    segment_files = []\n",
    "    for sample in samples:\n",
    "        ids.append(sample['id'])\n",
    "        audio_files.append(sample['audio_file'])\n",
    "        segment_files.append(sample['segment_file'])\n",
    "        \n",
    "    \n",
    "    return dict(batch=batch, lengths=lengths, \n",
    "                boundaries=boundaries, ids=ids, \n",
    "                audio_files=audio_files, \n",
    "                segment_files=segment_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ed003332",
   "metadata": {
    "cellId": "nry85isbeiodn1s7sal4io"
   },
   "outputs": [],
   "source": [
    "train_dataset = Dataset_res('train/', 'segments_edges_init/', 'manifest_silero_edges_train3.json', \n",
    "                              chars = 'segments_chars/', frames = 'secs/')\n",
    "val_dataset = Dataset_res('train/', 'segments_edges_init/', 'manifest_silero_edges_val1.json', \n",
    "                              chars = 'segments_chars/', frames = 'secs/')\n",
    "test_dataset = Dataset_res('test/', 'segments_edges_init_test/', 'manifest_test.json', \n",
    "                              chars = 'segments_chars/', frames = 'secs/')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=8, collate_fn = collate_fn_res)\n",
    "val_loader = DataLoader(val_dataset, shuffle=False, batch_size=8, collate_fn = collate_fn_res)\n",
    "test_loader = DataLoader(test_dataset, shuffle=True, batch_size=8, collate_fn = collate_fn_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6a994dd9",
   "metadata": {
    "cellId": "u5fydb386m7i8cb5xfo4td"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'segment_file': 'segments_edges_init/crowd/0/segments_chars/secs/77b380796d242cf5bc09cfb551cffecd.txt',\n",
       " 'id': '77b380796d242cf5bc09cfb551cffecd.txt',\n",
       " 'boundaries': [0.07375355450236964,\n",
       "  0.3201990521327014,\n",
       "  0.39602843601895743,\n",
       "  0.43394312796208534,\n",
       "  0.5287298578199053,\n",
       "  0.5666445497630332]}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "14df36b2",
   "metadata": {
    "cellId": "sd4d46qqeljzn00ctrnxhn"
   },
   "outputs": [],
   "source": [
    "path_to_save = 'save_results_path'\n",
    "path_results = os.path.join(path_to_save, 'Results')\n",
    "folders = ['train', 'val', 'test2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33236f14",
   "metadata": {
    "cellId": "6kriajd2mv8q2m1lj3bq9n"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c973264f",
   "metadata": {
    "cellId": "c5vgacnrn18pxs50wc5zl"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 980538/980538 [1:30:42<00:00, 180.16it/s]\n",
      "100%|██████████| 108957/108957 [09:00<00:00, 201.47it/s]\n",
      "100%|██████████| 11804/11804 [00:24<00:00, 475.87it/s]\n"
     ]
    }
   ],
   "source": [
    "result_dataframes = []\n",
    "metr = RMetrics1()\n",
    "for folder, dataset in zip(folders, [train_dataset, val_dataset, test_dataset]):\n",
    "\n",
    "    secs_preds = []\n",
    "    secs_trues = []\n",
    "\n",
    "    for i in tqdm(range(len(dataset))):\n",
    "        \n",
    "        idd = dataset[i]\n",
    "        true_file = idd['segment_file']\n",
    "        bound_true = idd['boundaries']\n",
    "        ids = idd['id']\n",
    "        \n",
    "#         bound_true = read_true_file(true_files[num])\n",
    "        bound_pred = read_pred_file(os.path.join(os.path.join(path_results, folder), ids))\n",
    "\n",
    "        secs_trues.append(bound_true)\n",
    "        secs_preds.append(bound_pred)\n",
    "   \n",
    "\n",
    "    precision, recall, f1, rval = metr.get_metrics_secs(secs_trues, secs_preds)\n",
    "    \n",
    "    datafr = pd.DataFrame([folder, \n",
    "                           precision, recall, \n",
    "                           f1, rval]).T.rename(columns = {0:'type', \n",
    "                                                          1:'precision',\n",
    "                                                          2:'recall', \n",
    "                                                          3:'f1', \n",
    "                                                          4:'rval'})\n",
    "    \n",
    "    result_dataframes.append(datafr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6c8e4542",
   "metadata": {
    "cellId": "f6zpboo9xwwxojdyu889k"
   },
   "outputs": [],
   "source": [
    "result_df = pd.concat(result_dataframes, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "74ddb383",
   "metadata": {
    "cellId": "zeeo4vvnz6lq3fvjeflncm"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>rval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train</td>\n",
       "      <td>0.843228</td>\n",
       "      <td>0.804486</td>\n",
       "      <td>0.823397</td>\n",
       "      <td>0.846699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>val</td>\n",
       "      <td>0.843317</td>\n",
       "      <td>0.809125</td>\n",
       "      <td>0.825862</td>\n",
       "      <td>0.849283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test2</td>\n",
       "      <td>0.805539</td>\n",
       "      <td>0.794868</td>\n",
       "      <td>0.800163</td>\n",
       "      <td>0.829379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    type precision    recall        f1      rval\n",
       "0  train  0.843228  0.804486  0.823397  0.846699\n",
       "1    val  0.843317  0.809125  0.825862  0.849283\n",
       "2  test2  0.805539  0.794868  0.800163  0.829379"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "02381dc4",
   "metadata": {
    "cellId": "2oxvkbximvofrvan3dx5a"
   },
   "outputs": [],
   "source": [
    "result_df.to_csv('results_golos.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e986101",
   "metadata": {
    "cellId": "0yj4uzwq5yyjemlq99e6rvr"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Yandex DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "notebookId": "df13bc40-6bd7-4064-84fd-724c5372ed39",
  "notebookPath": "Predict.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
